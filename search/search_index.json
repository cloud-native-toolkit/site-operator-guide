{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"apply/","text":"Apply \u00b6 After you've gotten a basic understanding of how things work by going through the labs in the Learn section, Apply is here with more detailed information on how to use these assets on an actual project.","title":"Overview"},{"location":"apply/#apply","text":"After you've gotten a basic understanding of how things work by going through the labs in the Learn section, Apply is here with more detailed information on how to use these assets on an actual project.","title":"Apply"},{"location":"apply/bill-of-material-reference/","text":"Bill of Material reference \u00b6 Warning This page has been relocated The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/ BOM metadata \u00b6 The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform BOM spec \u00b6 The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations BOM module definition \u00b6 A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules. BOM Module dependencies \u00b6 If the module depends on other modules, the relationships can be defined in the dependencies block. However, in most cases it is not necessary to explicitly define the dependencies. Through the module metadata, the iascable tool knows the required dependencies for each module and can \"auto-wire\" the modules together. If necessary, iascable will automatically add modules to the BOM if they are required to satisfy a required module dependency. If there are multiple instances of a dependent module defined in the BOM then iascable will \"auto-wire\" the dependency to the \"default\" dependent module. The \"default\" dependent module is the one that uses the default alias name OR has the default: true attribute added to it. If a default cannot be identified then ANOTHER instance of the module will be automatically added to the BOM. If this behavior is not desired then the desired dependent module can be referenced in the dependencies block. For example: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp The ibm-vpc-subnets module depends on ibm-vpc . An explicit declaration of the dependency is not required here though because the ibm-vpc module is the default instance and all of the ibm-vpc-subnets are auto-wired to that instance. (In fact the ibm-vpc module doesn't even need to be explicitly listed in the BOM in this case, but it is added for completeness.) The ibm-vpc-ocp module depends on ibm-vpc-subnets to identify where the cluster should be deployed. In this configuration, a default ibm-vpc-subnets instance has not been defined. As a result, iascable will automatically pull in 4th ibm-vpc-subnets instance to satisfy the dependency. This is probably not the desired result and we will want to explicitly define the dependency in the BOM. The updated BOM would look like the following: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp dependencies : - id : subnets ref : cluster_subnets The subnets identifier in the dependencies array refers to the dependency identifier in the module metadata for the ibm-vpc-ocp module. The cluster_subnets value refers to the alias of the target ibm-vpc-subnets module instance. Note: The only exception to iascable automatically pulling dependent modules into the BOM is if there are multiple module options that satisfy the dependency. In this case one of the modules that satisfies dependency must be explicitly added to the BOM. Otherwise the iascable build command will give an error that the dependency cannot be resolved. BOM Module variables \u00b6 The Bill of Materials also allows the module variables to be configured in a variables block. The variables block is an array of variable definitions. At a minimum the variable name must be provided. The available variable names are defined in the module metadata. For each variable, the following values can be provided: Field Description value The default value of the variable. This value will override the default in the module. scope The scope of the variable that defines how the variable will be handled in the global variable namespace. Allowed values are global or module . If the value is global the variable will be added as-is to the global namespace. If the value is module then the variable name will be prefixed with the module alias (e.g. the flavor variable in the cluster module would be named cluster_flavor with module scope and flavor with global scope). alias The alias name that should be given to the variable in the global variable namespace. This alias works in conjunction with the scope value. For example, if the name variable is set to global scope and alias of my_name then a variable named my_name will be added to the global variable namespace and the generated module terraform will map the my_name global variable to the name module variable ( name = var.my_name ) important Flag that indicates the variable should be presented to the user in the generated *.auto.tfvars file even though it has a default value. By default, only required fields (i.e. fields that don't have a default value) are presented to the user. Selectively, other variables can be exposed using this flag for significant configuration values. The objective is to balance flexibility of configuration options with the simplicity of a small number of required inputs Note: The module metadata defines how the outputs from the dependent modules should be wired into a module's input variables. It is not necessary to define any of the \"wired\" variables in the BOM. Example Bill of Material \u00b6 apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Bill of Material"},{"location":"apply/bill-of-material-reference/#bill-of-material-reference","text":"Warning This page has been relocated The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/","title":"Bill of Material reference"},{"location":"apply/bill-of-material-reference/#bom-metadata","text":"The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform","title":"BOM metadata"},{"location":"apply/bill-of-material-reference/#bom-spec","text":"The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations","title":"BOM spec"},{"location":"apply/bill-of-material-reference/#bom-module-definition","text":"A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules.","title":"BOM module definition"},{"location":"apply/bill-of-material-reference/#example-bill-of-material","text":"apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Example Bill of Material"},{"location":"apply/end-to-end-testing/","text":"End To End Testing for any Software Module \u00b6 The following provides step-by-step instructions for the end-to-end testing of software BOMs (e.g. Turbonomic) which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic Follow the steps to implement the end-to-end testing \u00b6 Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Clone the Git repo for the software (such as turbonomic) which needs to be tested end-to-end git clone https://github.com/IBM/automation-turbonomic.git Note Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. From the command-line, change directory to the the automation-solutions repository. Navigate to the folder containing the layers that will be generated. For Turbonomic the path is boms/software/turbonomic . Run the generate script to create the automation output. ./generate.sh The output will look something like the following: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200 -openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250 -turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note Every software layer which requires common layer such as gitops or storage as well as configuration will have a symbolic link to the file(s) in the shared location. Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. strategy : matrix : flavor : - ibm storage : - odf - portworx Add environment variables needed for this module in the verify-pr.yaml env : HOME : \"\" IBMCLOUD_API_KEY : \"\" The steps section represents a sequence of tasks that will be executed as part of job. Add the steps which needs to be executed in the sequence. Modify the 200-openshift-gitops BOM to support Gitea. (If you are using the shared gitops BOM then this step isn't necessary.) Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username } Copy the .mocks folder which has the configuration for BOM layer dependency. If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph Trigger the module build which will kick off the end-to-end test for the software to be tested. You can watch the progress from the GitHub Actions tab.","title":"End-To-End Testing"},{"location":"apply/end-to-end-testing/#end-to-end-testing-for-any-software-module","text":"The following provides step-by-step instructions for the end-to-end testing of software BOMs (e.g. Turbonomic) which can be replicated for any software such as CP4I, CP4D etc. Turbonomic Repo - https://github.com/IBM/automation-turbonomic","title":"End To End Testing for any Software Module"},{"location":"apply/end-to-end-testing/#follow-the-steps-to-implement-the-end-to-end-testing","text":"Checkout the Git repo from the https://github.com/cloud-native-toolkit/automation-solutions git clone https://github.com/cloud-native-toolkit/automation-solutions.git Clone the Git repo for the software (such as turbonomic) which needs to be tested end-to-end git clone https://github.com/IBM/automation-turbonomic.git Note Make sure the you keep the automation solutions and automation-turbonomic in the same level directory since we will be generating the files that will go directly to automation-turbonomic folder. Otherwise, you need to copy the files and manually move to automation-turbonomic folder. From the command-line, change directory to the the automation-solutions repository. Navigate to the folder containing the layers that will be generated. For Turbonomic the path is boms/software/turbonomic . Run the generate script to create the automation output. ./generate.sh The output will look something like the following: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 200 -openshift-gitops Writing output to: ../../../../automation-turbonomic Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: 250 -turbonomic-multicloud Writing output to: ../../../../automation-turbonomic Copying Files Copying Configuration Note Every software layer which requires common layer such as gitops or storage as well as configuration will have a symbolic link to the file(s) in the shared location. Navigate to the software (automation-turbonomic) and verify the files are generated as well as .github folder exist which is requires for the end-to-end test to run. Add the end to end test logic in the verify-workflow.yaml (automation-turbonomic.github\\workflows) of the Software module to be tested Below example strategy with do the end-to-end testing for the Turbonomic software on IBM Cloud infrastructure with the storage ODF and Portworx. strategy : matrix : flavor : - ibm storage : - odf - portworx Add environment variables needed for this module in the verify-pr.yaml env : HOME : \"\" IBMCLOUD_API_KEY : \"\" The steps section represents a sequence of tasks that will be executed as part of job. Add the steps which needs to be executed in the sequence. Modify the 200-openshift-gitops BOM to support Gitea. (If you are using the shared gitops BOM then this step isn't necessary.) Make sure generated main.tf is referencing the Gitea variables inside Gitops Module in the main.tf module \"gitops_repo\" { source = \"github.com/cloud-native-toolkit/terraform-tools-gitops?ref=v1.21.0\" branch = var.gitops_repo_branch debug = var.debug gitea_host = module.gitea.host gitea_org = module.gitea.org gitea_token = module.gitea.token gitea_username = module.gitea.username } Copy the .mocks folder which has the configuration for BOM layer dependency. If you have any specific dependency between layers, you can describe in the terragrunt.hcl Note You can also validate the dependency if its configured properly by launching the container (.launch.sh) and run the CLI terragrunt graph-dependencies which displays the dependency graph Trigger the module build which will kick off the end-to-end test for the software to be tested. You can watch the progress from the GitHub Actions tab.","title":"Follow the steps to implement the end-to-end testing"},{"location":"apply/module-metadata-reference/","text":"Module Metadata reference \u00b6 Warning This page is now in the reference section","title":"Module Metadata reference"},{"location":"apply/module-metadata-reference/#module-metadata-reference","text":"Warning This page is now in the reference section","title":"Module Metadata reference"},{"location":"apply/multi-catalog/","text":"","title":"Overview"},{"location":"apply/multi-catalog/ascent/","text":"Custom catalogs in ASCENT \u00b6 Coming soon","title":"Custom catalogs in ASCENT"},{"location":"apply/multi-catalog/ascent/#custom-catalogs-in-ascent","text":"Coming soon","title":"Custom catalogs in ASCENT"},{"location":"apply/multi-catalog/custom-catalog/","text":"Creating a Custom Catalog \u00b6 Coming soon","title":"Creating a catalog"},{"location":"apply/multi-catalog/custom-catalog/#creating-a-custom-catalog","text":"Coming soon","title":"Creating a Custom Catalog"},{"location":"apply/multi-catalog/iascable/","text":"Multiple catalogs with IasCable \u00b6 A core set of automation modules has been provided in the module catalog that list public modules from a number of different sources. However, there are situations where additional catalogs are needed (e.g. private modules). With IasCable it is possible to define a Bill of Material with modules from multiple catalogs and generate the automation appropriately. There are two ways to specify the catalog(s) that should be used by IasCable: Annotation in the Bill of Material Argument passed on the command line Note This information applies to IasCable v2.17.0 or higher Bill of Material annotation \u00b6 The Bill of Material provides an annotation area within the \"metadata\" section. The annotations are used to provide additional information for processing Bill of Material. (The full list of annotations and labels can be found in the Bill of Material reference .) In this case, an optional catalogUrls annotation has been provided to define the list of catalogs in which BOM modules can be found. The value is a comma-separated list of urls (either file:// or http(s)://). apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : my-custom-modules annotations : path : test catalogUrls : https://mymodules.someproject.com/index.yaml,https://othermodules.someproject.com/index.yaml spec : modules : - name : custom-module-1 - name : custom-module-2 Catalog URL argument \u00b6 The iascable build command provides a -c argument to pass in the catalog url(s). This argument can be provided multiple times to apply multiple catalogs, with subsequent catalogs taking precedence over previous ones. If not provided, the build uses the main catalog by default - https://modules.cloudnativetoolkit.dev/index.yaml . Note Only the catalog urls provided will be used. If you would like to use a custom catalog in addition to the main catalog then the main catalog will need to be included via the -c argument. iascable build -c https://modules.cloudnativetoolkit.dev/index.yaml -c https://mymodules.someproject.com/index.yaml -i my-custom-modules.yaml","title":"Generating automation"},{"location":"apply/multi-catalog/iascable/#multiple-catalogs-with-iascable","text":"A core set of automation modules has been provided in the module catalog that list public modules from a number of different sources. However, there are situations where additional catalogs are needed (e.g. private modules). With IasCable it is possible to define a Bill of Material with modules from multiple catalogs and generate the automation appropriately. There are two ways to specify the catalog(s) that should be used by IasCable: Annotation in the Bill of Material Argument passed on the command line Note This information applies to IasCable v2.17.0 or higher","title":"Multiple catalogs with IasCable"},{"location":"apply/multi-catalog/iascable/#bill-of-material-annotation","text":"The Bill of Material provides an annotation area within the \"metadata\" section. The annotations are used to provide additional information for processing Bill of Material. (The full list of annotations and labels can be found in the Bill of Material reference .) In this case, an optional catalogUrls annotation has been provided to define the list of catalogs in which BOM modules can be found. The value is a comma-separated list of urls (either file:// or http(s)://). apiVersion : cloud.ibm.com/v1alpha1 kind : BillOfMaterial metadata : name : my-custom-modules annotations : path : test catalogUrls : https://mymodules.someproject.com/index.yaml,https://othermodules.someproject.com/index.yaml spec : modules : - name : custom-module-1 - name : custom-module-2","title":"Bill of Material annotation"},{"location":"apply/multi-catalog/iascable/#catalog-url-argument","text":"The iascable build command provides a -c argument to pass in the catalog url(s). This argument can be provided multiple times to apply multiple catalogs, with subsequent catalogs taking precedence over previous ones. If not provided, the build uses the main catalog by default - https://modules.cloudnativetoolkit.dev/index.yaml . Note Only the catalog urls provided will be used. If you would like to use a custom catalog in addition to the main catalog then the main catalog will need to be included via the -c argument. iascable build -c https://modules.cloudnativetoolkit.dev/index.yaml -c https://mymodules.someproject.com/index.yaml -i my-custom-modules.yaml","title":"Catalog URL argument"},{"location":"concepts/","text":"Concepts \u00b6 Todo Complete this section. Topics should include: Overview Cloud environments Infrastructure as code Modular and layered composition Hybrid platform (OpenShift and Kubernetes) Operators Solution lifecycle CI/CD GitOps (including app of apps pattern) Helm (including chart dependencies) Toolkit concepts Modules Bill of Materials (BOM) dependencies configuration / variables In the simplest terms, the Toolkit delivers a set of packaged blocks of automation, called modules, that can be composed into a template, called a Bill of Materials, that describes a desired computing environment. The Bill of Materials can then be built and deployed to generate and run the automation code to create the desired computing environment. This section will explore some of the concepts behind how the toolkit works and the environments generated by some of the modules in the public modules catalog .","title":"Overview"},{"location":"concepts/#concepts","text":"Todo Complete this section. Topics should include: Overview Cloud environments Infrastructure as code Modular and layered composition Hybrid platform (OpenShift and Kubernetes) Operators Solution lifecycle CI/CD GitOps (including app of apps pattern) Helm (including chart dependencies) Toolkit concepts Modules Bill of Materials (BOM) dependencies configuration / variables In the simplest terms, the Toolkit delivers a set of packaged blocks of automation, called modules, that can be composed into a template, called a Bill of Materials, that describes a desired computing environment. The Bill of Materials can then be built and deployed to generate and run the automation code to create the desired computing environment. This section will explore some of the concepts behind how the toolkit works and the environments generated by some of the modules in the public modules catalog .","title":"Concepts"},{"location":"concepts/bom/","text":"Bill of Material Concepts \u00b6 Todo Compete this concepts page nested BOMs? A Bill of Material, often referred to as a BOM, is used to orchestrate a set of modules to define a required computing environment. What is a Bill of Material? \u00b6 A bill of material is a yaml file, that defines a computing environment by including the modules that will provide the automation to realize the required computing environment. The structure of the yaml file has been modelled after the Kubernetes Custom Resource Definition. Details of the yaml file syntax and content can be found in the Bill of Materials reference A bill of material does not need to specify every module that must be deployed because modules specify their dependencies. This allows the Toolkit tooling to resolve dependencies automatically unless: a dependency doesn't resolve to a specific module, such as an interface there are more complex dependency requirements, such as where a dependency may need to be shared between multiple modules in the BOM, rather than each module getting its own copy of its dependent modules Note Complex module dependency scenarios are covered in the Bill of Materials reference section A Bill of Materials allows you to provide module configuration using the variables property. In general you should think of a Bill of Materials as a template that can be used to create multiple instances of a desired environment. You might want to provide configuration data in the BOM for properties that need to apply to all instances of the environment created, such as the number of worker nodes for a BOM deploying a Kubernetes cluster for testing, but shouldn't include properties that need to change for each environment instance. Note Module configuration data is discussed more in the properties section How does a Bill of Material work? \u00b6 The Bill of Material specifies the modules that should be deployed to realize the required environment. To create the desired environment there are 3 steps needed: run the iascable tool to process the BOM [optionally] run the generated launch.sh script to enter the tooling VM or container environment run the generated apply.sh script to run the required automation Todo Add some comment about how the builder (Ascent) tooling handles these steps - hidden or visible to end user? Process the BOM \u00b6 The first process needed when deploying a Bill of Material is to resolve all the module dependencies. The iascable build command creates: a fully resolved BOM , where all module dependencies are fully resolved and included in the resolved BOM the launch.sh and apply.sh scripts the Terraform application from the modules specified in the fully resolved BOM to create the desired environment By default the iascable command line tool uses the Toolkit default module catalog , but additional catalog URLs can be specified in the BOM or as iascable command line arguments to allow private or 3rd party module catalogs to be used. Todo Provide links on how to include additional catalogues from the BOM reference page and iascable reference page once created If the BOM contains a module that the dependencies cannot be resolved, then an error will be reported and you will need to add the missing dependency to the BOM. Launching the standard Tooling runtime \u00b6 docs/getting-started/setup/setup.md As discussed in the Getting Started section there are a number of tools used when deploying an environment. To remove issues with version incompatibility and the differences between command line processors a standard environment should be used as a virtual machine (using multipass) or as a container (using Docker). The launch.sh script generated by the iascable build command provides a convenient way to launch the standard toolkit environment. It also ensures that any environment variables configured by the user are set in the virtual machine or container. Note Using environment variables to pass credentials into the module configuration is discussed in the properties section Running the automation to create the desired environment \u00b6 The apply.sh script is generated by the iascable build command. It is used to run the automation to deploy the desired environment, but before starting the deployment it ensures that all required configuration data is provided. Module configuration can be fulfilled from a number of sources: Module configuration can have default values the BOM can contain values for configuration environment variables can be configured a variables.yaml file can be use to provide configuration data If there is no value provided for any of the configuration properties of any module in the BOM then the apply script will prompt for the value to be entered on the command line. Once all the configuration data is fully provided then the apply script will start the Terraform application to realize the environment specified in the BOM by running the terraform init then terraform apply commands. Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Bill of Materials"},{"location":"concepts/bom/#bill-of-material-concepts","text":"Todo Compete this concepts page nested BOMs? A Bill of Material, often referred to as a BOM, is used to orchestrate a set of modules to define a required computing environment.","title":"Bill of Material Concepts"},{"location":"concepts/bom/#what-is-a-bill-of-material","text":"A bill of material is a yaml file, that defines a computing environment by including the modules that will provide the automation to realize the required computing environment. The structure of the yaml file has been modelled after the Kubernetes Custom Resource Definition. Details of the yaml file syntax and content can be found in the Bill of Materials reference A bill of material does not need to specify every module that must be deployed because modules specify their dependencies. This allows the Toolkit tooling to resolve dependencies automatically unless: a dependency doesn't resolve to a specific module, such as an interface there are more complex dependency requirements, such as where a dependency may need to be shared between multiple modules in the BOM, rather than each module getting its own copy of its dependent modules Note Complex module dependency scenarios are covered in the Bill of Materials reference section A Bill of Materials allows you to provide module configuration using the variables property. In general you should think of a Bill of Materials as a template that can be used to create multiple instances of a desired environment. You might want to provide configuration data in the BOM for properties that need to apply to all instances of the environment created, such as the number of worker nodes for a BOM deploying a Kubernetes cluster for testing, but shouldn't include properties that need to change for each environment instance. Note Module configuration data is discussed more in the properties section","title":"What is a Bill of Material?"},{"location":"concepts/bom/#how-does-a-bill-of-material-work","text":"The Bill of Material specifies the modules that should be deployed to realize the required environment. To create the desired environment there are 3 steps needed: run the iascable tool to process the BOM [optionally] run the generated launch.sh script to enter the tooling VM or container environment run the generated apply.sh script to run the required automation Todo Add some comment about how the builder (Ascent) tooling handles these steps - hidden or visible to end user?","title":"How does a Bill of Material work?"},{"location":"concepts/bom/#process-the-bom","text":"The first process needed when deploying a Bill of Material is to resolve all the module dependencies. The iascable build command creates: a fully resolved BOM , where all module dependencies are fully resolved and included in the resolved BOM the launch.sh and apply.sh scripts the Terraform application from the modules specified in the fully resolved BOM to create the desired environment By default the iascable command line tool uses the Toolkit default module catalog , but additional catalog URLs can be specified in the BOM or as iascable command line arguments to allow private or 3rd party module catalogs to be used. Todo Provide links on how to include additional catalogues from the BOM reference page and iascable reference page once created If the BOM contains a module that the dependencies cannot be resolved, then an error will be reported and you will need to add the missing dependency to the BOM.","title":"Process the BOM"},{"location":"concepts/bom/#launching-the-standard-tooling-runtime","text":"docs/getting-started/setup/setup.md As discussed in the Getting Started section there are a number of tools used when deploying an environment. To remove issues with version incompatibility and the differences between command line processors a standard environment should be used as a virtual machine (using multipass) or as a container (using Docker). The launch.sh script generated by the iascable build command provides a convenient way to launch the standard toolkit environment. It also ensures that any environment variables configured by the user are set in the virtual machine or container. Note Using environment variables to pass credentials into the module configuration is discussed in the properties section","title":"Launching the standard Tooling runtime"},{"location":"concepts/bom/#running-the-automation-to-create-the-desired-environment","text":"The apply.sh script is generated by the iascable build command. It is used to run the automation to deploy the desired environment, but before starting the deployment it ensures that all required configuration data is provided. Module configuration can be fulfilled from a number of sources: Module configuration can have default values the BOM can contain values for configuration environment variables can be configured a variables.yaml file can be use to provide configuration data If there is no value provided for any of the configuration properties of any module in the BOM then the apply script will prompt for the value to be entered on the command line. Once all the configuration data is fully provided then the apply script will start the Terraform application to realize the environment specified in the BOM by running the terraform init then terraform apply commands. Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Running the automation to create the desired environment"},{"location":"concepts/gitops/","text":"GitOps Concepts \u00b6 Todo Complete this concepts page","title":"GitOps"},{"location":"concepts/gitops/#gitops-concepts","text":"Todo Complete this concepts page","title":"GitOps Concepts"},{"location":"concepts/module/","text":"Module concepts \u00b6 What is a module? \u00b6 A Toolkit module represents a bundle of automation that delivers a defined outcome, wrapped with metadata. Breaking down complex environment setup steps into small modules, allows better testing and reuse of individual modules. A module should follow the Separation of Concerns design principle , where each module is well defined and addresses a single concern or task without overlapping with the functionality of other modules. A Bill of Materials is used to combine the automation provided by Toolkit modules to create a desired environment. The modules are the building blocks and the Bill of Materials are the instructions on how to combine the building blocks to create the desired outcome The automation used in the toolkit is based on Terraform , but the module metadata and tooling provided by the Toolkit ensures that the end user of the Toolkit does not need to know how to use Terraform and does not get exposed to the underlying Terraform automation. There are currently 2 flavours of Toolkit module: Terraform module GitOps module Both flavours of module use Terraform, but a Terraform module uses Terraform to implement the required automation steps, whilst the GitOps modules use Terraform to make changes to the GitOps source control repository, then the GitOps application, ArgoCD , will actually make the required changes based on the content of the source control repository. See the GitOps concept page for more details. How does a module work? \u00b6 The Toolkit adds a layer of abstraction over the underlying Terraform automation to allow modules to be combined to define a required computing environment, typically a cloud based environment. The modules are combined using a Bill of Material yaml file. The Toolkit layer of abstraction includes: module data : a set of properties, including the module a name, ID, a description of what the module does, the platform where the module can run and if the module is a terraform or GitOps module configuration : a set of properties that controls the output of the module. This allows a module to be configured to perform its task in a specific way. E.g. when installing software on a Kubernetes cluster, you may want to specify the namespace where the software is installed when the module is run rather than hard coding the namespace into the module automation dependencies : modules that must be run before this module can perform its work Configuration \u00b6 A module can specify a set of input and output properties. The input properties allow the functionality of the module automation to be customized at run time. The output properties allow a module to make information it generates available to subsequent modules, such as a password generated during an application install. There is more detail about how properties are used in the Toolkit on the properties concepts page . Dependencies \u00b6 A module may need other modules to have completed their automation tasks before it can perform its own task. The Toolkit provides a way to allow a module to specify a set of other modules as dependencies. This ensures modules run in the correct order. The dependency system also allows output properties of previously run modules to be linked to the input properties of a module. Interfaces \u00b6 Some modules can result in similar outcomes, such as installing an OpenShift cluster. The modules all result in a running OpenShift cluster, but they may be on different cloud providers or on bare metal systems. This can cause an issue if a module wants to specify an OpenShift cluster as a dependency, as which of the many modules that provide an OpenShift cluster should be specified as the dependency? To overcome this challenge, the Toolkit allows interfaces to be defined that specify a set of output properties that must be produced by any module that implements the interface. A module can specify an interface as a dependency rather than a specific module. An example of an interface is a cluster . Looking at the cluster section of the module catalog , you see there are 7 modules that provide a cluster. Actually there are 6 modules that will install a cluster and the IBM OpenShift Login module that allows you to use an existing cluster. More details about how a module is implemented can be found in the module reference section. Catalog \u00b6 A module needs to be included in a catalog for it to be available to a Bill of Material. There is a default module catalog provided by the Toolkit, and there is a defined process to add new modules to the catalog. However, there may be circumstances where modules may need to stay private, so a private module catalog will be needed. A collection of modules may be managed outside the Toolkit community, but be publicly available in a separate module catalog. The Toolkit tooling allows modules from multiple catalogues to be used. Todo Add link to the instructions for adding new module to Toolkit catalog in section above Does the builder UI allow multiple catalogues to be used? Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Module"},{"location":"concepts/module/#module-concepts","text":"","title":"Module concepts"},{"location":"concepts/module/#what-is-a-module","text":"A Toolkit module represents a bundle of automation that delivers a defined outcome, wrapped with metadata. Breaking down complex environment setup steps into small modules, allows better testing and reuse of individual modules. A module should follow the Separation of Concerns design principle , where each module is well defined and addresses a single concern or task without overlapping with the functionality of other modules. A Bill of Materials is used to combine the automation provided by Toolkit modules to create a desired environment. The modules are the building blocks and the Bill of Materials are the instructions on how to combine the building blocks to create the desired outcome The automation used in the toolkit is based on Terraform , but the module metadata and tooling provided by the Toolkit ensures that the end user of the Toolkit does not need to know how to use Terraform and does not get exposed to the underlying Terraform automation. There are currently 2 flavours of Toolkit module: Terraform module GitOps module Both flavours of module use Terraform, but a Terraform module uses Terraform to implement the required automation steps, whilst the GitOps modules use Terraform to make changes to the GitOps source control repository, then the GitOps application, ArgoCD , will actually make the required changes based on the content of the source control repository. See the GitOps concept page for more details.","title":"What is a module?"},{"location":"concepts/module/#how-does-a-module-work","text":"The Toolkit adds a layer of abstraction over the underlying Terraform automation to allow modules to be combined to define a required computing environment, typically a cloud based environment. The modules are combined using a Bill of Material yaml file. The Toolkit layer of abstraction includes: module data : a set of properties, including the module a name, ID, a description of what the module does, the platform where the module can run and if the module is a terraform or GitOps module configuration : a set of properties that controls the output of the module. This allows a module to be configured to perform its task in a specific way. E.g. when installing software on a Kubernetes cluster, you may want to specify the namespace where the software is installed when the module is run rather than hard coding the namespace into the module automation dependencies : modules that must be run before this module can perform its work","title":"How does a module work?"},{"location":"concepts/module/#configuration","text":"A module can specify a set of input and output properties. The input properties allow the functionality of the module automation to be customized at run time. The output properties allow a module to make information it generates available to subsequent modules, such as a password generated during an application install. There is more detail about how properties are used in the Toolkit on the properties concepts page .","title":"Configuration"},{"location":"concepts/module/#dependencies","text":"A module may need other modules to have completed their automation tasks before it can perform its own task. The Toolkit provides a way to allow a module to specify a set of other modules as dependencies. This ensures modules run in the correct order. The dependency system also allows output properties of previously run modules to be linked to the input properties of a module.","title":"Dependencies"},{"location":"concepts/module/#interfaces","text":"Some modules can result in similar outcomes, such as installing an OpenShift cluster. The modules all result in a running OpenShift cluster, but they may be on different cloud providers or on bare metal systems. This can cause an issue if a module wants to specify an OpenShift cluster as a dependency, as which of the many modules that provide an OpenShift cluster should be specified as the dependency? To overcome this challenge, the Toolkit allows interfaces to be defined that specify a set of output properties that must be produced by any module that implements the interface. A module can specify an interface as a dependency rather than a specific module. An example of an interface is a cluster . Looking at the cluster section of the module catalog , you see there are 7 modules that provide a cluster. Actually there are 6 modules that will install a cluster and the IBM OpenShift Login module that allows you to use an existing cluster. More details about how a module is implemented can be found in the module reference section.","title":"Interfaces"},{"location":"concepts/module/#catalog","text":"A module needs to be included in a catalog for it to be available to a Bill of Material. There is a default module catalog provided by the Toolkit, and there is a defined process to add new modules to the catalog. However, there may be circumstances where modules may need to stay private, so a private module catalog will be needed. A collection of modules may be managed outside the Toolkit community, but be publicly available in a separate module catalog. The Toolkit tooling allows modules from multiple catalogues to be used. Todo Add link to the instructions for adding new module to Toolkit catalog in section above Does the builder UI allow multiple catalogues to be used? Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Catalog"},{"location":"concepts/variables/","text":"Configuration Data \u00b6 Note This is an advanced topic, not needed to use the Toolkit, but contains useful information if you need to debug an issue or want to create your own modules The Toolkit modules are created to allow their functionality to be customized at runtime. The customization is controlled using configuration data, variables , passed to a module when it is run. Ultimately each module will run Terraform to perform its automation tasks, so all the configuration data is made available using standard Terraform input variables . Understanding how the underlying Terraform works can help when debugging an issue or of you want to create a new module, but a consumer should not need to touch any Terraform files nor understand how Terraform works to use the Toolkit to deploy a Bill of Materials. Module defined variables \u00b6 When a module is create the developer defines the variables needed for the module to work (input) and also the data the module can expose to subsequent modules (output). Input variables \u00b6 Input variables are defined in the module.yaml file in the root folder of the module implementation. Each input variable is defined using multiple properties: name : the name of the variable type : the data type of the variable description : the description of what data the variable holds optional : if the variable must be provided or if it is optional important : a flag to mark the variable as important default : the default value for the variable scope : the scope of the variable (global or module) moduleRef : a reference to a dependent module output variable to bind to this variable. At deploy times all non-optional variables must have a value. The default value will be used if provided. If there is no default value and the variable isn't marked as optional then a value will need to be provided at deploy time. The important flag is used to indicate variables that are key for the module functionality and will need to be verified by the end user at deploy time rather than just accept any default value. The scope of a variable can be either global or module . When a variable has module scope it is referred to using a combination of the module name followed by the variable name - <module name>-<variable name> . E.g. If I have a module called gitops-repo and within the module there is an input variable called project : with no scope defined or module scope specified the variable is referred to as gitops-repo-project with global scope specified the variable is referred to as project A Bill of Material (BOM) can alias a module input variable to rename it and/or change the scope. E.g. If multiple modules in a BOM all have a namespace input variable and the BOM requires them all to be set to the same namespace. In the BOM the namespace variable for each module can be aliased to be a global scope variable called deploy_namespace . At deploy time the user is only required to provide a value for the single global variable deploy_namespace and all modules will assign that value to their namespace variable. Todo Add a link to the task showing where module variables are aliased The other scenario when a variable is aliased is when a variable will hold a secret which the user may provide as an environment variable. Environment variable names containing the dash or minus ( - ) character can cause issues in Linux so is best avoided. A variable alias can be used to rename a variable to use underscore characters ( _ ) instead of the dashes. Note In a module implementation the input variables are also defined in the terraform file variables.tf . This is where each variable is given a type and optionally properties, such as sensitive for variable containing a secret that shouldn't be displayed in output. Output variables \u00b6 A module's output variables are defined in the Terraform output.tf file. They are not defined in the module.yaml file. If a module specifies another module as a dependency, then they can access the output variables of the dependent module Providing Variables for a deployment \u00b6 There are a number of ways the configuration data can be provided to a module: default values in the module definition default values in the Bill of Materials taking output from previously run modules environment variables variables.yaml file command line prompts at deploy time The variables are processed from top to bottom in the above list with the last set value being used for any property. E.g. If a value is provided as an environment variable and in the variables.yaml file, then the value from the variables.yaml file will be used. When deploying a bill of materials using the scripts generated by the Toolkit, environment variables can be specified in a file called credentials.properties. The variables defined in this will be be automatically added to the environment. The Toolkit will generate the required Terraform variable files automatically, there should be no need to touch Terraform files when deploying a Bill of Materials. Any configuration data required by any module in the Bill of Materials that is not provided as: a default value in the Module definition a default value in the Bill of Material definition a mapped output from a dependant module an environment variable in the variables.yaml file will need to be provided on the command line when deploying. Where is the best place to set a value? \u00b6 One of the principles of the Toolkit is to make consumability as easy as possible, so where possible sensible default values should be provided. The module creator should set default values for the module, where they make sense. When creating a Bill of Material there may be values that can be set, or it may be appropriate to set a collection of configuration properties to a common value or configuration variable that the user will need to input. E.g. if you needed to deploy a number of services to the same namespace. You could alias the variables to a common variable, so the user only needs to set a single configuration property for namespace that all the modules will use rather than making the user provide the same namespace value multiple times. You don't want to set default values that would restrict the Bill of Material to a single instance. The Bill of Materials should be a template that cen be used to deployed multiple environments. Set default values for properties that you want to be common across all the deployed environments in the Bill of Materials. Properties that are unique to a specific environment should be set in the variables.yaml file or the credentials.property ( environment variables ) files. You will also need to add any module variables with the important flag set as these need to be explicitly set or you will be prompted to confirm the values at deploy time. Any property that contains sensitive data, such as a password, token or other secret, should be placed in environment variables (credentials.properties) and all other property values should be placed in variables.yaml. Warning Environment variables should not contain the - character, so be sure to alias any variable names to use _ instead Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Configuration data"},{"location":"concepts/variables/#configuration-data","text":"Note This is an advanced topic, not needed to use the Toolkit, but contains useful information if you need to debug an issue or want to create your own modules The Toolkit modules are created to allow their functionality to be customized at runtime. The customization is controlled using configuration data, variables , passed to a module when it is run. Ultimately each module will run Terraform to perform its automation tasks, so all the configuration data is made available using standard Terraform input variables . Understanding how the underlying Terraform works can help when debugging an issue or of you want to create a new module, but a consumer should not need to touch any Terraform files nor understand how Terraform works to use the Toolkit to deploy a Bill of Materials.","title":"Configuration Data"},{"location":"concepts/variables/#module-defined-variables","text":"When a module is create the developer defines the variables needed for the module to work (input) and also the data the module can expose to subsequent modules (output).","title":"Module defined variables"},{"location":"concepts/variables/#input-variables","text":"Input variables are defined in the module.yaml file in the root folder of the module implementation. Each input variable is defined using multiple properties: name : the name of the variable type : the data type of the variable description : the description of what data the variable holds optional : if the variable must be provided or if it is optional important : a flag to mark the variable as important default : the default value for the variable scope : the scope of the variable (global or module) moduleRef : a reference to a dependent module output variable to bind to this variable. At deploy times all non-optional variables must have a value. The default value will be used if provided. If there is no default value and the variable isn't marked as optional then a value will need to be provided at deploy time. The important flag is used to indicate variables that are key for the module functionality and will need to be verified by the end user at deploy time rather than just accept any default value. The scope of a variable can be either global or module . When a variable has module scope it is referred to using a combination of the module name followed by the variable name - <module name>-<variable name> . E.g. If I have a module called gitops-repo and within the module there is an input variable called project : with no scope defined or module scope specified the variable is referred to as gitops-repo-project with global scope specified the variable is referred to as project A Bill of Material (BOM) can alias a module input variable to rename it and/or change the scope. E.g. If multiple modules in a BOM all have a namespace input variable and the BOM requires them all to be set to the same namespace. In the BOM the namespace variable for each module can be aliased to be a global scope variable called deploy_namespace . At deploy time the user is only required to provide a value for the single global variable deploy_namespace and all modules will assign that value to their namespace variable. Todo Add a link to the task showing where module variables are aliased The other scenario when a variable is aliased is when a variable will hold a secret which the user may provide as an environment variable. Environment variable names containing the dash or minus ( - ) character can cause issues in Linux so is best avoided. A variable alias can be used to rename a variable to use underscore characters ( _ ) instead of the dashes. Note In a module implementation the input variables are also defined in the terraform file variables.tf . This is where each variable is given a type and optionally properties, such as sensitive for variable containing a secret that shouldn't be displayed in output.","title":"Input variables"},{"location":"concepts/variables/#output-variables","text":"A module's output variables are defined in the Terraform output.tf file. They are not defined in the module.yaml file. If a module specifies another module as a dependency, then they can access the output variables of the dependent module","title":"Output variables"},{"location":"concepts/variables/#providing-variables-for-a-deployment","text":"There are a number of ways the configuration data can be provided to a module: default values in the module definition default values in the Bill of Materials taking output from previously run modules environment variables variables.yaml file command line prompts at deploy time The variables are processed from top to bottom in the above list with the last set value being used for any property. E.g. If a value is provided as an environment variable and in the variables.yaml file, then the value from the variables.yaml file will be used. When deploying a bill of materials using the scripts generated by the Toolkit, environment variables can be specified in a file called credentials.properties. The variables defined in this will be be automatically added to the environment. The Toolkit will generate the required Terraform variable files automatically, there should be no need to touch Terraform files when deploying a Bill of Materials. Any configuration data required by any module in the Bill of Materials that is not provided as: a default value in the Module definition a default value in the Bill of Material definition a mapped output from a dependant module an environment variable in the variables.yaml file will need to be provided on the command line when deploying.","title":"Providing Variables for a deployment"},{"location":"concepts/variables/#where-is-the-best-place-to-set-a-value","text":"One of the principles of the Toolkit is to make consumability as easy as possible, so where possible sensible default values should be provided. The module creator should set default values for the module, where they make sense. When creating a Bill of Material there may be values that can be set, or it may be appropriate to set a collection of configuration properties to a common value or configuration variable that the user will need to input. E.g. if you needed to deploy a number of services to the same namespace. You could alias the variables to a common variable, so the user only needs to set a single configuration property for namespace that all the modules will use rather than making the user provide the same namespace value multiple times. You don't want to set default values that would restrict the Bill of Material to a single instance. The Bill of Materials should be a template that cen be used to deployed multiple environments. Set default values for properties that you want to be common across all the deployed environments in the Bill of Materials. Properties that are unique to a specific environment should be set in the variables.yaml file or the credentials.property ( environment variables ) files. You will also need to add any module variables with the important flag set as these need to be explicitly set or you will be prompted to confirm the values at deploy time. Any property that contains sensitive data, such as a password, token or other secret, should be placed in environment variables (credentials.properties) and all other property values should be placed in variables.yaml. Warning Environment variables should not contain the - character, so be sure to alias any variable names to use _ instead Todo Add links to reference pages for BOM and Module + links to the appropriate tasks and tutorials for concepts covered on this page","title":"Where is the best place to set a value?"},{"location":"contribute/","text":"Contribute \u00b6 Todo Complete this section Community guidelines for contributing to the project Overview of process of creating a new module (actual instructions should be tasks) request a new public module develop and test a module locally host a private module catalog github automation used to build, test and publish a new module","title":"Overview"},{"location":"contribute/#contribute","text":"Todo Complete this section Community guidelines for contributing to the project Overview of process of creating a new module (actual instructions should be tasks) request a new public module develop and test a module locally host a private module catalog github automation used to build, test and publish a new module","title":"Contribute"},{"location":"getting-started/","text":"Getting Started - Overview \u00b6 The Toolkit provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Tutorial section. From there, the Tasks section provides more detailed guides to move from \"Hello World\" to real world. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Overview"},{"location":"getting-started/#getting-started-overview","text":"The Toolkit provides automation and guidance to help operations and SRE teams provision and manage complex, hybrid-cloud environments from pilot to production. This site is structured to give hands-on experience first through controlled lab exercises in the Tutorial section. From there, the Tasks section provides more detailed guides to move from \"Hello World\" to real world. In terms of the overall lifecycle, this guide is focused on Day 0 provisioning and Day 2 operations. The information on the develop site covers Day 1 functions.","title":"Getting Started - Overview"},{"location":"getting-started/best-practices/","text":"Best Practices \u00b6 Todo Complete this page","title":"Best practices"},{"location":"getting-started/best-practices/#best-practices","text":"Todo Complete this page","title":"Best Practices"},{"location":"getting-started/setup/","text":"Setup you learning and development environment \u00b6 The automation used within the toolkit uses a number of open source tools and utilities. To avoid issues with different operating systems, command line interpreters (shells) and versions of the various tools, it is recommended that a container or virtual machine is used, so you will have a verified working environment for the automation to run in. Some of the tools used are: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Todo what about other cloud CLIs? Supported runtime environments \u00b6 The developers do not have the bandwidth to run and test will all possible combinations of runtimes across different operating systems, so these are the recommended, tested OS/runtime environments: Linux Mac OS Windows Docker Engine Docker Desktop Multipass Windows Subsystem for Linux running Ubuntu 22.04.1 LTS image with Docker Engine installed Docker \u00b6 Docker Desktop provides a container environment for MacOS. It is free to use for non-commercial uses, but requires a license for commercial use. If the license isn't an issue then this is the simplest, recommended option to use. If running on Linux or Windows Subsystem for Linux, then docker engine is the recommended option to use. Multipass \u00b6 Multipass is a free virtual machine environment for Windows, MacOS and Linux to run Ubuntu images. Only Multipass on MacOS is supported within this documentation. Warning Some users have reported DNS resolution issues when using Multipass with some VPN clients, such as Cisco Anywhere. There are workaround solutions covered in the setup tutorial. Additional unsupported options \u00b6 There are some additional environments that are used within the community, but these are not supported and cannot be guaranteed to work. There are also some known issues with the environments listed below: podman colima Podman \u00b6 Podman is an open source tool, free to use, that provides much the same functionality as Docker Desktop. There are some known issues with Podman: When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module. Colima \u00b6 Colima is an open source container engine for Intel or Arm based Mac systems. It is free to use, but there are some known issues with Colima: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Setup instructions \u00b6 Instructions for setting up your chosen container or VM environment are covered in the first tutorial","title":"Setup"},{"location":"getting-started/setup/#setup-you-learning-and-development-environment","text":"The automation used within the toolkit uses a number of open source tools and utilities. To avoid issues with different operating systems, command line interpreters (shells) and versions of the various tools, it is recommended that a container or virtual machine is used, so you will have a verified working environment for the automation to run in. Some of the tools used are: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Todo what about other cloud CLIs?","title":"Setup you learning and development environment"},{"location":"getting-started/setup/#supported-runtime-environments","text":"The developers do not have the bandwidth to run and test will all possible combinations of runtimes across different operating systems, so these are the recommended, tested OS/runtime environments: Linux Mac OS Windows Docker Engine Docker Desktop Multipass Windows Subsystem for Linux running Ubuntu 22.04.1 LTS image with Docker Engine installed","title":"Supported runtime environments"},{"location":"getting-started/setup/#docker","text":"Docker Desktop provides a container environment for MacOS. It is free to use for non-commercial uses, but requires a license for commercial use. If the license isn't an issue then this is the simplest, recommended option to use. If running on Linux or Windows Subsystem for Linux, then docker engine is the recommended option to use.","title":"Docker"},{"location":"getting-started/setup/#multipass","text":"Multipass is a free virtual machine environment for Windows, MacOS and Linux to run Ubuntu images. Only Multipass on MacOS is supported within this documentation. Warning Some users have reported DNS resolution issues when using Multipass with some VPN clients, such as Cisco Anywhere. There are workaround solutions covered in the setup tutorial.","title":"Multipass"},{"location":"getting-started/setup/#additional-unsupported-options","text":"There are some additional environments that are used within the community, but these are not supported and cannot be guaranteed to work. There are also some known issues with the environments listed below: podman colima","title":"Additional unsupported options"},{"location":"getting-started/setup/#setup-instructions","text":"Instructions for setting up your chosen container or VM environment are covered in the first tutorial","title":"Setup instructions"},{"location":"getting-started/whats-new/","text":"What's new \u00b6","title":"What's new"},{"location":"getting-started/whats-new/#whats-new","text":"","title":"What's new"},{"location":"learn/","text":"Learn \u00b6 Warning This content will move to the Tutorial section Learn how to use the Toolkit to provision and customize cloud resources, be it infrastructure or software, across a number of hosting environments. TechZone Accelerator Toolkit Builder \u00b6 The TechZone Accelerator Toolkit Builder is a user interface that provides a catalog of common reference architectures and wizards to combine and customize them for use in hybrid-cloud environments. Coming soon IasCable \u00b6 The logic for generating automation templates in TechZone Accelerator Toolkit Builder is provided by IasCable. IasCable has been built to convert a listing of the required capabilities of an architecture (referred to as a Bill of Materials) and generate the necessary automation to bring the architecture to life. For most use cases, TechZone Automation Builder is the recommended way to work with the automation. However, IasCable provides a more advanced level of control over the definition of the architecture. Learn IasCable - Learn how to define your own Bill of Material and use IasCable to generate the automation.","title":"Overview"},{"location":"learn/#learn","text":"Warning This content will move to the Tutorial section Learn how to use the Toolkit to provision and customize cloud resources, be it infrastructure or software, across a number of hosting environments.","title":"Learn"},{"location":"learn/#techzone-accelerator-toolkit-builder","text":"The TechZone Accelerator Toolkit Builder is a user interface that provides a catalog of common reference architectures and wizards to combine and customize them for use in hybrid-cloud environments. Coming soon","title":"TechZone Accelerator Toolkit Builder"},{"location":"learn/#iascable","text":"The logic for generating automation templates in TechZone Accelerator Toolkit Builder is provided by IasCable. IasCable has been built to convert a listing of the required capabilities of an architecture (referred to as a Bill of Materials) and generate the necessary automation to bring the architecture to life. For most use cases, TechZone Automation Builder is the recommended way to work with the automation. However, IasCable provides a more advanced level of control over the definition of the architecture. Learn IasCable - Learn how to define your own Bill of Material and use IasCable to generate the automation.","title":"IasCable"},{"location":"learn/iascable/","text":"Getting started with IasCable \u00b6 The getting started section provides you three initial getting started with IasCable and TechZone Accelerator Toolkit . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Technology Zone Accelerator Toolkit . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab 3: Use TechZone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud Lab 4: Develop an own GitOps module","title":"Overview"},{"location":"learn/iascable/#getting-started-with-iascable","text":"The getting started section provides you three initial getting started with IasCable and TechZone Accelerator Toolkit . Lab 1: Getting started with the basics Deploy a Virtual Private Cloud in IBM Cloud with IasCable and initial Terraform modules from Technology Zone Accelerator Toolkit . Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud Lab 3: Use TechZone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud Lab 4: Develop an own GitOps module","title":"Getting started with IasCable"},{"location":"learn/iascable/lab1/","text":"Lab 1: Getting started with the basics \u00b6 As a starting point use [IasCable](https://github.com/cloud-native-toolkit/iascable) . 1. The IasCable framework \u00b6 Let us look at the basic components of the IasCable framework. Bill of Material and Modules \u00b6 The IasCable uses a Bill of Material and Modules (from the TechZone Accelerator Toolkit ), this is an important point to understand. Bill of Material (BOM) and Modules are the heart of the framework to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. In a simplified way, we can say a BOM is specified by modules it uses, variables you can define and providers you can define. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. We can simply say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case, the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework. Realize Technology Zone Accelerator Toolkit with Bill of Material and Modules \u00b6 Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the created Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies. 2. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 3. Step-by-step example setup \u00b6 This is a step by step set up example to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview Step 1: Install CLI \u00b6 curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh Step 2: Verify the installation \u00b6 iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ] Step 3: Create a Bill of Materials (BOM) file \u00b6 nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets Step 4: Execute following command \u00b6 iascable build -i firstbom.yaml Step 5: Verify the created content \u00b6 \u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is the table which contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective is to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group Step 6: Execute the terraform init command \u00b6 Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init Step 7: Execute the terraform plan command \u00b6 Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Step 8: Execute the terraform apply \u00b6 Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed. Step 9: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed. 4. Summary \u00b6 The IasCable and the Modules (from the TechZone Accelerator Toolkit) are all from the Cloud Native Toolkit providing a good concept for a framework. This helps to provide reusable components to install and configure cloud infrastructure. What you have learned so far is just helping you to get started, there is more to learn.","title":"Lab 1: The basics"},{"location":"learn/iascable/lab1/#lab-1-getting-started-with-the-basics","text":"As a starting point use [IasCable](https://github.com/cloud-native-toolkit/iascable) .","title":"Lab 1: Getting started with the basics"},{"location":"learn/iascable/lab1/#1-the-iascable-framework","text":"Let us look at the basic components of the IasCable framework.","title":"1. The IasCable framework"},{"location":"learn/iascable/lab1/#bill-of-material-and-modules","text":"The IasCable uses a Bill of Material and Modules (from the TechZone Accelerator Toolkit ), this is an important point to understand. Bill of Material (BOM) and Modules are the heart of the framework to realize the objective to build an installable component infrastructure based on components from a catalog of available modules. Please visit the linked resources for more details. In a simplified way, we can say a BOM is specified by modules it uses, variables you can define and providers you can define. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case the related modules will be included by the framework, as far as I understand. We can simply say a BOM is specified by modules it uses. In addition you have the option to use variables and providers definitions related to a BOM specification. It is good to know that Modules can have dependencies to other modules, if this is the case, the related modules will be included by the framework, as far as I understand. Here is a simplified overview diagram of the dependencies: Here is a simplified activity diagram that shows the activities carried out by the user and the IasCable framework.","title":"Bill of Material and Modules"},{"location":"learn/iascable/lab1/#realize-technology-zone-accelerator-toolkit-with-bill-of-material-and-modules","text":"Simplified we can say, we have two basic roles in that context: Creators (Architect, Developer or Operator) defining Bill of Materials to create Terraform automation for creating specific infrastructure based on reusing existing Terraform modules. Consumers who using the created Terraform automation based on the BOM definition. And we have two major elements to define and create the needed Terraform automation. The BOM configures IasCable to point to right Terraform modules in a module catalog to create the Terraform automation code. IasCable is uses Terraform modules to create a Terraform automation which will be consumed. The following diagram shows some high level dependencies.","title":"Realize Technology Zone Accelerator Toolkit with Bill of Material and Modules"},{"location":"learn/iascable/lab1/#2-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"2. Pre-requisites for the example"},{"location":"learn/iascable/lab1/#3-step-by-step-example-setup","text":"This is a step by step set up example to create a Virtual Private Cloud with three Subnets on IBM Cloud. 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups Simplified architecture overview","title":"3. Step-by-step example setup"},{"location":"learn/iascable/lab1/#step-1-install-cli","text":"curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Step 1: Install CLI"},{"location":"learn/iascable/lab1/#step-2-verify-the-installation","text":"iascable build --help Example output: Configure ( and optionally deploy ) the iteration zero assets Options: --version Show version number [ boolean ] --help Show help [ boolean ] -u, --catalogUrl The url of the module catalog. Can be https:// or file:/ protocol. [ default: \"https://modules.cloudnativetoolkit.dev/index.yaml\" ] -i, --input The path to the bill of materials to use as input [ array ] -r, --reference The reference BOM to use for the build [ array ] -o, --outDir The base directory where the command output will be written [ default: \"./output\" ] --platform Filter for the platform ( kubernetes or ocp4 ) --provider Filter for the provider ( ibm or k8s ) --tileLabel The label for the tile. Required if you want to generate the tile metadata. --name The name used to override the module name in the bill of material. [ array ] --tileDescription The description of the tile. --flattenOutput, --flatten Flatten the generated output into a single directory ( i.e. remove the terraform folder ) . [ boolean ] --debug Flag to turn on more detailed output message [ boolean ]","title":"Step 2: Verify the installation"},{"location":"learn/iascable/lab1/#step-3-create-a-bill-of-materialsbom-file","text":"nano firstbom.yaml Copy following content into the new file: apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets","title":"Step 3: Create a Bill of Materials(BOM) file"},{"location":"learn/iascable/lab1/#step-4-execute-following-command","text":"iascable build -i firstbom.yaml","title":"Step 4: Execute following command"},{"location":"learn/iascable/lab1/#step-5-verify-the-created-content","text":"\u251c\u2500\u2500 firstbom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u2514\u2500\u2500 ibm-vpc.md \u2502 \u251c\u2500\u2500 ibm-vpc.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh output folder The folder output contains all the content created by the iascable build command output/ibm-vpc folder The folder ibm-vpc is the name we used in our own BOM file. Let us call that folder project folder, which was defined in meta data. metadata : name : ibm-vpc output/ibm-vpc/terraform folder This is the table which contains the list of files in the terraform folder. Filename Content output/ibm-vpc/terraform/main.tf Here you see a number of modules defined including the defined ibm-vpc and ibm-vpc-subnets from the BOM file. output/ibm-vpc/terraform/providers.tf Simply contains the needed cloud provider information. In that case what we need to specify for IBM Cloud . output/ibm-vpc/terraform/variables.ft Contains the specification for the used variable in the main.tf or other Terraform files. output/ibm-vpc/terraform/version.ft Contains the specification for the used Terraform provider sources and versions. In that case only IBM is listed. output/ibm-vpc/terraform/ibm-vpc.auto.tfvars That file can be used to configure the variable values. (maybe add to .gitignore) During the execution of terraform plan and terraform apply you will be ask for input, if you didn't specify that values. The output/launch.sh file That script downloads and starts a container on your local machine. The objective is to ensure that the right environment is used for applying the Terraform configuration. It attaches the local path to the container as a volume. Note: Need to ensure you have a container engine on your machine. Best Docker! Because by default is uses Docker. Attach doesn't work for podman on macOS. The output/ibm-vpc/apply.sh file That file converts an existing variable.yaml file or variable in the BOM file to a variables.tf and then executes terraform init and terraform apply . The output/ibm-vpc/destroy.sh file That file simply executes the terraform init and terraform destroy -auto-approve commands. The output/ibm-vpc/dependencies.dot file That file contains the dependencies which can be visualized for example with Graphviz Online . Example: The output/bom.yaml file That file was created by our own BOM file . That file now contains all needed variables. These variables are also reflected in the output/ibm-vpc/terraform/variables.ft file. Here is the content of the newly created bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc spec : modules : - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group","title":"Step 5: Verify the created content"},{"location":"learn/iascable/lab1/#step-6-execute-the-terraform-init-command","text":"Navigate to the output/ibm-vpc/terraform folder and execute the terraform init command. cd output/ibm-vpc/terraform terraform init","title":"Step 6: Execute the terraform init command"},{"location":"learn/iascable/lab1/#step-7-execute-the-terraform-plan-command","text":"Execute the terraform plan command. terraform plan Here you can see the interaction: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example .","title":"Step 7: Execute the terraform plan  command"},{"location":"learn/iascable/lab1/#step-8-execute-the-terraform-apply","text":"Execute the terraform apply command. terraform apply -auto-approve Input of your variables: var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview of the resources which will be created: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create < = read ( data resources ) Terraform will perform the following actions: # module.ibm-vpc.data.ibm_is_security_group.base will be read during apply # (config refers to values not yet known) < = data \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_egress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_egress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.allow_internal_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"allow_internal_ingress\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"allow-internal-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ingress[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ingress\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_rdp[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_rdp\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-rdp\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 3389 + port_min = 3389 + source_port_max = 3389 + source_port_min = 3389 } } # module.ibm-vpc.ibm_is_network_acl_rule.deny_external_ssh[0] will be created + resource \"ibm_is_network_acl_rule\" \"deny_external_ssh\" { + action = \"deny\" + before = ( known after apply ) + destination = \"0.0.0.0/0\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"deny-external-ssh\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"0.0.0.0/0\" + tcp { + port_max = 22 + port_min = 22 + source_port_max = 22 + source_port_min = 22 } } # module.ibm-vpc.ibm_is_security_group.base[0] will be created + resource \"ibm_is_security_group\" \"base\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-base\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + rules = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.10\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.cse_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"cse_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.11\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_http[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_http\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + tcp { + port_max = 80 + port_min = 80 } } # module.ibm-vpc.ibm_is_security_group_rule.default_inbound_ping[0] will be created + resource \"ibm_is_security_group_rule\" \"default_inbound_ping\" { + direction = \"inbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"0.0.0.0/0\" + rule_id = ( known after apply ) + icmp { + type = 8 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_1[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_1\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.7\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[0] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_security_group_rule.private_dns_2[1] will be created + resource \"ibm_is_security_group_rule\" \"private_dns_2\" { + direction = \"outbound\" + group = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + protocol = ( known after apply ) + related_crn = ( known after apply ) + remote = \"161.26.0.8\" + rule_id = ( known after apply ) + udp { + port_max = 53 + port_min = 53 } } # module.ibm-vpc.ibm_is_vpc.vpc[0] will be created + resource \"ibm_is_vpc\" \"vpc\" { + address_prefix_management = \"auto\" + classic_access = false + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = \"default-vpc-default\" + default_routing_table = ( known after apply ) + default_routing_table_name = \"default-vpc-default\" + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = \"default-vpc-default\" + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.nacl-tag[0] will be created + resource \"ibm_resource_tag\" \"nacl-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.ibm_resource_tag.sg-tag[0] will be created + resource \"ibm_resource_tag\" \"sg-tag\" { + acccount_id = ( known after apply ) + id = ( known after apply ) + resource_id = ( known after apply ) + tag_type = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_is_vpc.vpc will be read during apply # (config refers to values not yet known) < = data \"ibm_is_vpc\" \"vpc\" { + classic_access = ( known after apply ) + crn = ( known after apply ) + cse_source_addresses = ( known after apply ) + default_network_acl = ( known after apply ) + default_network_acl_crn = ( known after apply ) + default_network_acl_name = ( known after apply ) + default_routing_table = ( known after apply ) + default_routing_table_name = ( known after apply ) + default_security_group = ( known after apply ) + default_security_group_crn = ( known after apply ) + default_security_group_name = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + security_group = ( known after apply ) + status = ( known after apply ) + subnets = ( known after apply ) + tags = ( known after apply ) } # module.ibm-vpc-subnets.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.ibm-vpc-subnets.ibm_is_network_acl.subnet_acl[0] will be created + resource \"ibm_is_network_acl\" \"subnet_acl\" { + crn = ( known after apply ) + id = ( known after apply ) + name = \"default-vpc-subnet-default\" + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + tags = ( known after apply ) + vpc = ( known after apply ) + rules { + action = ( known after apply ) + destination = ( known after apply ) + direction = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = ( known after apply ) + source = ( known after apply ) + subnets = ( known after apply ) + icmp { + code = ( known after apply ) + type = ( known after apply ) } + tcp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } + udp { + port_max = ( known after apply ) + port_min = ( known after apply ) + source_port_max = ( known after apply ) + source_port_min = ( known after apply ) } } } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[0] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-ingress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[1] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"166.8.0.0/14\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[2] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"inbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-ingress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"161.26.0.0/16\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[3] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"10.0.0.0/8\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-egress-internal\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[4] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"166.8.0.0/14\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-roks-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_network_acl_rule.acl_rule[5] will be created + resource \"ibm_is_network_acl_rule\" \"acl_rule\" { + action = \"allow\" + before = ( known after apply ) + destination = \"161.26.0.0/16\" + direction = \"outbound\" + href = ( known after apply ) + id = ( known after apply ) + ip_version = ( known after apply ) + name = \"default-vpc-subnet-default-allow-vse-egress\" + network_acl = ( known after apply ) + protocol = ( known after apply ) + rule_id = ( known after apply ) + source = \"10.0.0.0/8\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[0] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default01\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-1\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[1] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default02\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-2\" } # module.ibm-vpc-subnets.ibm_is_subnet.vpc_subnets[2] will be created + resource \"ibm_is_subnet\" \"vpc_subnets\" { + access_tags = ( known after apply ) + available_ipv4_address_count = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + ip_version = \"ipv4\" + ipv4_cidr_block = ( known after apply ) + name = \"default-vpc-subnet-default03\" + network_acl = ( known after apply ) + public_gateway = ( known after apply ) + resource_controller_url = ( known after apply ) + resource_crn = ( known after apply ) + resource_group = ( known after apply ) + resource_group_name = ( known after apply ) + resource_name = ( known after apply ) + resource_status = ( known after apply ) + routing_table = ( known after apply ) + status = ( known after apply ) + tags = ( known after apply ) + total_ipv4_address_count = 256 + vpc = ( known after apply ) + zone = \"us-south-3\" } # module.ibm-vpc-subnets.null_resource.print_names will be created + resource \"null_resource\" \"print_names\" { + id = ( known after apply ) } # module.resource_group.data.ibm_resource_group.resource_group will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_group\" \"resource_group\" { + account_id = ( known after apply ) + created_at = ( known after apply ) + crn = ( known after apply ) + id = ( known after apply ) + is_default = ( known after apply ) + name = \"default\" + payment_methods_url = ( known after apply ) + quota_id = ( known after apply ) + quota_url = ( known after apply ) + resource_linkages = ( known after apply ) + state = ( known after apply ) + teams_url = ( known after apply ) + updated_at = ( known after apply ) } # module.resource_group.data.ibm_resource_tag.resource_group_tags will be read during apply # (config refers to values not yet known) < = data \"ibm_resource_tag\" \"resource_group_tags\" { + id = ( known after apply ) + resource_id = ( known after apply ) + tags = ( known after apply ) } # module.resource_group.null_resource.resource_group will be created + resource \"null_resource\" \"resource_group\" { + id = ( known after apply ) + triggers = ( known after apply ) } # module.resource_group.null_resource.wait_for_sync will be created + resource \"null_resource\" \"wait_for_sync\" { + id = ( known after apply ) } # module.resource_group.random_uuid.tag will be created + resource \"random_uuid\" \"tag\" { + id = ( known after apply ) + result = ( known after apply ) } # module.resource_group.module.clis.data.external.setup-binaries will be read during apply # (config refers to values not yet known) < = data \"external\" \"setup-binaries\" { + id = ( known after apply ) + program = [ + \"bash\" , + \".terraform/modules/resource_group.clis/scripts/setup-binaries.sh\" , ] + query = { + \"bin_dir\" = \"/Users/thomassuedbroecker/Downloads/dev/iascable-starting-point/examples/output/ibm-vpc/terraform/bin2\" + \"clis\" = \"yq,jq,igc\" + \"uuid\" = ( known after apply ) } + result = ( known after apply ) } # module.resource_group.module.clis.null_resource.print will be created + resource \"null_resource\" \"print\" { + id = ( known after apply ) } # module.resource_group.module.clis.random_string.uuid will be created + resource \"random_string\" \"uuid\" { + id = ( known after apply ) + length = 16 + lower = true + min_lower = 0 + min_numeric = 0 + min_special = 0 + min_upper = 0 + number = false + numeric = ( known after apply ) + result = ( known after apply ) + special = false + upper = false } Plan: 36 to add, 0 to change, 0 to destroy. \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Final result of the creation Apply complete! Resources: 36 added, 0 changed, 0 destroyed.","title":"Step 8: Execute the terraform apply"},{"location":"learn/iascable/lab1/#step-9-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete the terraform.tfstate and the .terraform.lock.hcl files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to ensure to provide the IBM Cloud API Key, the region and the resource group name. var.ibmcloud_api_key the value of ibmcloud_api_key Enter a value: xxxx var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: us-south var.resource_group_name The name of the resource group Enter a value: default Output overview: \u2577 \u2502 Warning: Experimental feature \"module_variable_optional_attrs\" is active \u2502 \u2502 on .terraform/modules/ibm-vpc-subnets/version.tf line 10 , in terraform: \u2502 10 : experiments = [ module_variable_optional_attrs ] \u2502 \u2502 Experimental features are subject to breaking changes in \u2502 future minor or patch releases, based on feedback. \u2502 \u2502 If you have feedback on the design of this feature, please \u2502 open a GitHub issue to discuss it. \u2575 \u2577 \u2502 Warning: Argument is deprecated \u2502 \u2502 with module.resource_group.module.clis.random_string.uuid, \u2502 on .terraform/modules/resource_group.clis/main.tf line 15 , in resource \"random_string\" \"uuid\" : \u2502 15 : number = false \u2502 \u2502 Use numeric instead. \u2502 \u2502 ( and one more similar warning elsewhere ) \u2575 Destroy complete! Resources: 36 destroyed.","title":"Step 9: Execute the terraform destroy command"},{"location":"learn/iascable/lab1/#4-summary","text":"The IasCable and the Modules (from the TechZone Accelerator Toolkit) are all from the Cloud Native Toolkit providing a good concept for a framework. This helps to provide reusable components to install and configure cloud infrastructure. What you have learned so far is just helping you to get started, there is more to learn.","title":"4. Summary"},{"location":"learn/iascable/lab2/","text":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud \u00b6 The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc 1. Pre-requisites for the example \u00b6 Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud 2. Step-by-step setup example \u00b6 This is a step by step setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview Step 1: Write the Bill of Material BOM file \u00b6 nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1 Step 2: Build the project based on Bill of Material BOM file \u00b6 iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output Step 3: Verify the created files and folders \u00b6 tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we can find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online . Step 4: Execute the terraform init command \u00b6 Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init Step 5: Execute the terraform apply command \u00b6 Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4 Step 6: Execute the terraform destroy command \u00b6 Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed. 3. Summary \u00b6 When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"Lab 2: Infrastructure"},{"location":"learn/iascable/lab2/#lab-2-use-iascable-to-create-a-vpc-and-a-red-hat-openshift-cluster-on-ibm-cloud","text":"The following list represents the modules which are referenced in the example IBM ROKS Bill of Materials for IasCable . IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc","title":"Lab 2: Use IasCable to create a VPC and a Red Hat OpenShift cluster on IBM Cloud"},{"location":"learn/iascable/lab2/#1-pre-requisites-for-the-example","text":"Following tools need to be installed on your local computer to follow the step by step instructions. Terraform Git That is the cloud environment we will use. IBM Cloud","title":"1. Pre-requisites for the example"},{"location":"learn/iascable/lab2/#2-step-by-step-setup-example","text":"This is a step by step setup to create a Virtual Private Cloud and an IBM Cloud managed Red Hat OpenShift cluster . 1 x Virtual Private Cloud 3 x Subnets 2 x Access Control Lists 1 x Routing Table 2 x Security Groups 3 x Public Gateway 1 x Virtual Private Endpoint Gateway 1 x Red Hat OpenShift cluster 3 x Worker Nodes one in each zone 1 x Default worker pool 1 x Cloud Object Storage Simplified architecture overview","title":"2. Step-by-step setup example"},{"location":"learn/iascable/lab2/#step-1-write-the-bill-of-material-bom-file","text":"nano my-vpc-roks-bom.yaml Copy and past the following content into the my-vpc-roks-bom.yaml file. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets - name : ibm-vpc-gateways - name : ibm-ocp-vpc variables : - name : worker_count value : 1","title":"Step 1: Write the Bill of Material BOM file"},{"location":"learn/iascable/lab2/#step-2-build-the-project-based-on-bill-of-material-bom-file","text":"iascable build -i my-vpc-roks-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks Writing output to: ./output","title":"Step 2: Build the project based on Bill of Material BOM file"},{"location":"learn/iascable/lab2/#step-3-verify-the-created-files-and-folders","text":"tree Output: . \u251c\u2500\u2500 my-vpc-roks-bom.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 launch.sh \u2514\u2500\u2500 my-ibm-vpc-roks \u251c\u2500\u2500 apply.sh \u251c\u2500\u2500 bom.yaml \u251c\u2500\u2500 dependencies.dot \u251c\u2500\u2500 destroy.sh \u2514\u2500\u2500 terraform \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2514\u2500\u2500 ibm-vpc.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 my-ibm-vpc-roks.auto.tfvars \u251c\u2500\u2500 providers.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4 directories, 17 files You can find details of the created files and folders also in IasCable starting point GitHub project and that blog post . In the newly created bom.yaml file we can find more detailed information about modules we are going to use. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks spec : modules : - name : ibm-ocp-vpc alias : cluster version : v1.15.4 variables : - name : worker_count value : 1 - name : ibm-vpc alias : ibm-vpc version : v1.16.0 - name : ibm-vpc-gateways alias : ibm-vpc-gateways version : v1.9.0 - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 - name : ibm-resource-group alias : resource_group version : v3.2.16 - name : ibm-object-storage alias : cos version : v4.0.3 variables : - name : region type : string description : The IBM Cloud region where the cluster will be/has been installed. - name : ibmcloud_api_key type : string description : The IBM Cloud api token - name : worker_count type : number description : >- The number of worker nodes that should be provisioned for classic infrastructure defaultValue : 1 - name : cluster_flavor type : string description : The machine type that will be provisioned for classic infrastructure defaultValue : bx2.4x16 - name : ibm-vpc-subnets__count type : number description : The number of subnets that should be provisioned defaultValue : 3 - name : resource_group_name type : string description : The name of the resource group (Network) IBM VPC ibm-vpc (Network) IBM VPC Subnets ibm-vpc-subnets (Network) IBM Cloud VPC Public Gateway ibm-vpc-gateways This module makes use of the output from other modules: * Resource group - github.com/cloud-native-toolkit/terraform-ibm-resource-group.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git (Cluster) IBM OpenShift VPC cluster ibm-ocp-vpc This module makes use of the output from other modules: * Object Storage - github.com/cloud-native-toolkit/terraform-ibm-object-storage.git * VPC - github.com/cloud-native-toolkit/terraform-ibm-vpc.git * Subnet - github.com/cloud-native-toolkit/terraform-ibm-vpc.git Added modules based on module dependencies: (IAM) IBM Resource Group ibm-resource-group (Storage) IBM Object Storage ibm-object-storage We can verify the dependencies with the dependencies.dot content for example in Graphviz Online .","title":"Step 3: Verify the created files and folders"},{"location":"learn/iascable/lab2/#step-4-execute-the-terraform-init-command","text":"Navigate to the output/my-ibm-vpc-roks/terraform folder and execute the terraform init command. cd output/my-ibm-vpc-roks/terraform terraform init","title":"Step 4: Execute the terraform init command"},{"location":"learn/iascable/lab2/#step-5-execute-the-terraform-apply-command","text":"Execute the terraform apply command. terraform apply -auto-approve Note: You can create an IBM Cloud API Key with following command: ibmcloud iam api-key-create iascable-example . Input of your variables: var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: Finally you should get this output in your terminal. ... Apply complete! Resources: 56 added, 0 changed, 0 destroyed. Following files were created, that you should delete because these will used for the deletion or update of the resources. I added those files in my case to .gitignore . example/output/my-ibm-vpc-roks/terraform/.terraform.lock.hcl example/output/my-ibm-vpc-roks/terraform/clis-debug.log example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin-key.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/admin.pem example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_admin_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/.kube/b88790957171731697722dc07f0f923283278cf784b9fce792b831afbab8d83e_default-cluster_k8sconfig/config.yml example/output/my-ibm-vpc-roks/terraform/bin2/.igc-release example/output/my-ibm-vpc-roks/terraform/bin2/igc example/output/my-ibm-vpc-roks/terraform/bin2/jq example/output/my-ibm-vpc-roks/terraform/bin2/yq3 example/output/my-ibm-vpc-roks/terraform/bin2/yq4","title":"Step 5: Execute the terraform apply  command"},{"location":"learn/iascable/lab2/#step-6-execute-the-terraform-destroy-command","text":"Note: Ensure you didn't delete created files before. To destroy the provisioned resources, run the following: terraform destroy -auto-approve You need to provide the IBM Cloud API Key, the region and the resource group name again, because we didn't save those values in variables. var.ibmcloud_api_key The IBM Cloud api token Enter a value: XXX var.region The IBM Cloud region where the cluster will be/has been installed. Enter a value: eu-de var.resource_group_name The name of the resource group Enter a value: default Output: The final output should be: Destroy complete! Resources: 56 destroyed.","title":"Step 6: Execute the terraform destroy command"},{"location":"learn/iascable/lab2/#3-summary","text":"When you use IasCable and the Modules it is much easier to setup infrastructure on a cloud using Terraform . The Modules containing a lot of pre-work and IasCable creates automated an awesome starting point for you. Surely you still should understand what are you creating and the final architecture you will be produce. Especially the pre-work for the example we use which divides the worker nodes over 3 zones, does pre-configurations of rules for the security groups and many more. So I looking forward now to get started with the GitOps topic which is also a part of the Modules in that framework.","title":"3. Summary"},{"location":"learn/iascable/lab3/","text":"Lab 3: Use TechZone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud \u00b6 Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The TechZone Accelerator Toolkit project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the TechZone Accelerator Toolkit catalog does provide. We need to know the outline for the cloud architecture on the cloud environment we are going to use. As I said the TechZone Accelerator Toolkit catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed TechZone Accelerator Toolkit Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helpful scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find these in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example 1. Define an outline of the target architecture \u00b6 This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud 2. Identify the needed TechZone Accelerator Toolkit Terraform modules for the target architecture \u00b6 Let us first define which TechZone Accelerator Toolkit Terraform modules we are going to use for our custom BOM file specification. The TechZone Accelerator Toolkit project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure 1. Configuration of GitOps \u00b6 IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap 2. Cloud infrastructure/services resources related \u00b6 IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage 3. Write a customized BOM to combine the modules \u00b6 Step 1: Write the Bill of Material BOM file \u00b6 Now we combine the existing Terraform modules we got from the TechZone Accelerator Toolkit catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization. 4. Use IasCable to create the scaffolding for a Terraform project \u00b6 Step 1: Install colima container engine and start the container engine \u00b6 Example for an installation of colima on macOS. brew install docker colima colima start Step 2: Create a terraform project based on Bill of Material BOM file \u00b6 Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output Step 3: Copy helper bash scripts into the output folder \u00b6 cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output Step 4: Start the tools container provided by the IasCable \u00b6 Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh 5. Use the IasCable tools container to execute the Terraform modules \u00b6 Step 1 (inside the container): In the running container verify the mapped resources \u00b6 ~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it \u00b6 sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd 6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration \u00b6 Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume \u00b6 All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file . 7. Destroy the environment on IBM Cloud \u00b6 Step 1 (inside the container): Destroy the created IBM Cloud resources \u00b6 All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed. 8. Summary \u00b6 We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Technology Zone Accelerator Toolkit project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The TechZone Accelerator Toolkit project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Technology Zone Accelerator Toolkit with Terraform and contribute to the project.","title":"Lab 3: GitOps"},{"location":"learn/iascable/lab3/#lab-3-use-techzone-accelerator-toolkit-and-iascable-to-setup-gitops-on-a-red-hat-openshift-cluster-in-a-virtual-private-cloud-on-ibm-cloud","text":"Our objective is to create a customized initial GitOps setup in an IBM Cloud environment. The TechZone Accelerator Toolkit project and IasCable CLI do provide an awesome way to eliminate writing Terraform modules for various clouds such as IBM Cloud , AWS or Azure to create and configure resources. We are going to reuse Terraform modules which the TechZone Accelerator Toolkit catalog does provide. We need to know the outline for the cloud architecture on the cloud environment we are going to use. As I said the TechZone Accelerator Toolkit catalog does provide the reuse of existing Terraform modules, which we use by just combining by writing a \" Bill of Material file \" and configure the variables for the related Terraform modules ( example link to the GitOps Terraform module ) when it is needed. We will not write any Terraform code , we will only combine existing Terraform modules and configure them using IasCable BOM files! In that scenario we will use IBM Cloud with a Virtual Private Cloud and a Red Hat OpenShift cluster with Argo CD installed and integrated with a GitHub project. These are the major sections: Define an outline of the target architecture Identify the needed TechZone Accelerator Toolkit Terraform modules for the target architecture Write a customized BOM to combine the modules Use IasCable to create the scaffolding for a Terraform project Use the IasCable tools container to execute the Terraform modules Note: Depending on the container engine you are going to use on your computer, you maybe have to copy the Terraform project inside the running tools container, because of access right restrictions to access mapped local volumes to the running containers. That is the reason why I wrote some helpful scripts to simplify the copy and deletion of the Terraform code mapped to the local volume of our computer. You can find these in the current helper bash automation script in the GitHub project . IasCable does suggest to use Docker or Colima Apply the Terraform modules to create the environment in IBM Cloud and backup the Terraform state to the local computer. Destroy the environment on IBM Cloud. Summary You can the access source code in the GitHub project I created. The project is under Apache-2.0 license . git clone https://github.com/thomassuedbroecker/iascable-vpc-openshift-argocd.git cd example","title":"Lab 3: Use TechZone Accelerator Toolkit and IasCable to setup GitOps on a Red Hat OpenShift Cluster in a Virtual Private Cloud on IBM Cloud"},{"location":"learn/iascable/lab3/#1-define-an-outline-of-the-target-architecture","text":"This is our simplified target architecture for our objective to create a customized setup in an IBM Cloud environment for GitOps. Configuration of GitOps in Red Hat OpenShift We will use two operators: * Red at OpenShift GitOps operator * We will create one [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) with the [Red at OpenShift GitOps operator](https://github.com/redhat-developer/gitops-operator), that [ArgoCD instance](https://argo-cd.readthedocs.io/en/stable/) will bin initial configured by a newly created GitHub project configure by a [Cloud Native Toolkit template](https://github.com/cloud-native-toolkit/terraform-tools-gitops/tree/main/template) for GitOps repositories. Red at OpenShift Pipelines operator . There will be no initial setup for a Tekton pipeline at the moment. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud","title":"1. Define an outline of the target architecture"},{"location":"learn/iascable/lab3/#2-identify-the-needed-techzone-accelerator-toolkit-terraform-modules-for-the-target-architecture","text":"Let us first define which TechZone Accelerator Toolkit Terraform modules we are going to use for our custom BOM file specification. The TechZone Accelerator Toolkit project points to the Automated Solutions project which contains several starting points for various setups, which can be used as a starting point. In our case we have two major areas for Terraform modules we want to use: Configuration of GitOps IBM Cloud infrastructure","title":"2. Identify the needed TechZone Accelerator Toolkit Terraform modules for the target architecture"},{"location":"learn/iascable/lab3/#1-configuration-of-gitops","text":"IBM OpenShift login ocp-login - login to existing OpenShift cluster GitOps repo gitops-repo - creates the GitOps Repo ArgoCD Bootstrap argocd-bootstrap","title":"1. Configuration of GitOps"},{"location":"learn/iascable/lab3/#2-cloud-infrastructureservices-resources-related","text":"IBM VPC ibm-vpc IBM VPC Subnets ibm-vpc-subnets IBM Cloud VPC Public Gateway ibm-vpc-gateways IBM OpenShift VPC cluster ibm-ocp-vpc IBM Object Storage ibm-object-storage","title":"2. Cloud infrastructure/services resources related"},{"location":"learn/iascable/lab3/#3-write-a-customized-bom-to-combine-the-modules","text":"","title":"3. Write a customized BOM to combine the modules"},{"location":"learn/iascable/lab3/#step-1-write-the-bill-of-material-bom-file","text":"Now we combine the existing Terraform modules we got from the TechZone Accelerator Toolkit catalog and we specify the variables in the BOM file we need to reflect our target architecture. Note: When we going to use these variables, we must keep in mind that we need to use the names of the variables defined in the Terraform modules and we should use alias: ibm-vpc to define the prefix in the BOM file. The BOM file for our architecture is divided in 3 main sections. Virtual Private Cloud Red Hat OpenShift Cluster (ROKS) GitOps We need to create an IBM Cloud API key and an Personal Access Token for the GitHub account . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : my-ibm-vpc-roks-argocd spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.0 variables : - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-sample\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.5 variables : - name : name value : \"tsued-gitops\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_gitops\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops\" - name : repo value : \"iascable-gitops\" The BOM will result later in following overall dependencies for the used Terraform modules after the usage of IasCable . The dependencies are given in automatic created dependencies.dot file later. Note: You can use GraphvizOnline for the visualization.","title":"Step 1: Write the Bill of Material BOM file"},{"location":"learn/iascable/lab3/#4-use-iascable-to-create-the-scaffolding-for-a-terraform-project","text":"","title":"4. Use IasCable to create the scaffolding for a Terraform project"},{"location":"learn/iascable/lab3/#step-1-install-colima-container-engine-and-start-the-container-engine","text":"Example for an installation of colima on macOS. brew install docker colima colima start","title":"Step 1: Install colima container engine and start the container engine"},{"location":"learn/iascable/lab3/#step-2-create-a-terraform-project-based-on-bill-of-material-bom-file","text":"Version iascable --version Output: 2 .14.1 Build iascable build -i my-vpc-roks-argocd-bom.yaml Output: Loading catalog from url: https://modules.cloudnativetoolkit.dev/index.yaml Name: my-ibm-vpc-roks-argocd Writing output to: ./output","title":"Step 2: Create a terraform project based on Bill of Material BOM file"},{"location":"learn/iascable/lab3/#step-3-copy-helper-bash-scripts-into-the-output-folder","text":"cp helper-tools-create-container-workspace.sh ./output cp helper-tools-execute-apply-and-backup-result.sh ./output cp helper-tools-execute-destroy-and-delete-backup.sh ./output","title":"Step 3: Copy helper bash scripts into the output folder"},{"location":"learn/iascable/lab3/#step-4-start-the-tools-container-provided-by-the-iascable","text":"Note: At the moment we need to change and save the launch.sh script a bit. Open the launch.sh script. cd output nano launch.sh Delete the -u \"${UID}\" parameter Before ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -u \" ${ UID } \" -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } After the change ${ DOCKER_CMD } run -itd --name ${ CONTAINER_NAME } -v \" ${ SRC_DIR } :/terraform\" -v \"workspace- ${ AUTOMATION_BASE } :/workspaces\" ${ ENV_FILE } -w /terraform ${ DOCKER_IMAGE } Execute the launch.sh script sh launch.sh","title":"Step 4: Start the tools container provided by the IasCable"},{"location":"learn/iascable/lab3/#5-use-the-iascable-tools-container-to-execute-the-terraform-modules","text":"","title":"5. Use the IasCable tools container to execute the Terraform modules"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-in-the-running-container-verify-the-mapped-resources","text":"~/src $ ls helper-tools-create-container-workspace.sh helper-tools-execute-apply-and-backup-result.sh helper-tools-execute-destroy-and-delete-backup.sh launch.sh my-ibm-vpc-roks-argocd","title":"Step 1 (inside the container): In the running container verify the mapped resources"},{"location":"learn/iascable/lab3/#step-2-inside-the-container-create-a-workspace-folder-in-your-container-and-copy-your-iascable-project-into-it","text":"sh helper-tools-create-container-workspace.sh ls /home/devops/workspace The following tasks are automated in the helper bash script helper-tools-create-container-workspace.sh I wrote. Creates a workspace folder Copies the Terraform project from the mapped volume folder to the workspace folder Output: You can see the copied Terraform project folder inside the container. my-ibm-vpc-roks-argocd","title":"Step 2 (inside the container): Create a workspace folder in your container and copy your IasCable project into it"},{"location":"learn/iascable/lab3/#6-apply-the-terraform-modules-to-create-the-environment-in-ibm-cloud-and-backup-terraform-configuration","text":"","title":"6. Apply the Terraform modules to create the environment in IBM Cloud and backup Terraform configuration"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-execute-the-applysh-and-backup-the-result-into-the-mapped-volume","text":"All these tasks are automated in the helper bash script I wrote. sh helper-tools-execute-apply-and-backup-result.sh The script helper-tools-execute-apply-and-backup-result.sh does following: Navigate to the created workspace Execute apply.sh script List the created resources Copy current start to mapped volume Interactive output: As we see in the output the values we inserted in our custom BOM file are now used as the default values. In our example we only need to insert the values for: gitops-repo_token ibmcloud_api_key resource_group_name region Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXX > Provide a value for 'ibmcloud_api_key' : > XXX Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Output: Move on with the setup and apply Terraform. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes After a while you should get following output. Apply complete! Resources: 91 added, 0 changed, 0 destroyed. Major resources which were created: Cloud infrastructure/services resources 1 x VPC 1 x Subnet 4 Security groups Two were created during the subnet creation and two are related to the created Red Hat OpenShift cluster. 1 x Virtual Private Endpoint 1 x Public Gateway 2 x Access Control Lists One was created for the VPC module and one during the creation by the subnet module. 1 x Routing Table 1 x Red Hat OpenShift Cluster 1 x Object Storage Cluster and GitOps configuration Red Hat OpenShift GitOps operator and Red Hat OpenShift Pipelines operator GitHub project as ArgoCD repository Preconfigure ArgoCD project The invoked apply.sh script will create following files or folders: Inside the tools container: a temporary workspace/my-ibm-vpc-roks-argocd/variables.yaml.tmp file a workspace/my-ibm-vpc-roks-argocd/variables.yaml file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tf file a workspace/my-ibm-vpc-roks-argocd/terraform/variables.tfvars file several folders .kube , .terraform , .tmp , bin2 , docs Then it creates a terraform.tfvars file based on the entries you gave and executes init and apply command from Terraform. Be aware that the IBM Cloud access key information and GitHub access token are saved in text format in the output/my-ibm-vpc-roks-argocd/terraform/terraform.tfvars file! Don't share this in a public GitHub repository. On GitHub it creates a GitHub private project which contains preconfigure ArgoCD resource provided by cloud native toolkit Note: Here you can sample of the content of an example for a generated variables.yaml file link and here you can find an example for the created BOM file .","title":"Step 1 (inside the container): Execute the apply.sh and backup the result into the mapped volume"},{"location":"learn/iascable/lab3/#7-destroy-the-environment-on-ibm-cloud","text":"","title":"7. Destroy the environment on IBM Cloud"},{"location":"learn/iascable/lab3/#step-1-inside-the-container-destroy-the-created-ibm-cloud-resources","text":"All these tasks are automated in the helper bash script I wrote. Note: Ensure you didn't delete created Terraform files before. sh helper-tools-execute-destroy-and-delete-backup.sh * The script helper-tools-execute-destroy-and-delete-backup.sh does following: Navigate to workspace Execute destroy.sh Navigate to the mapped volume Copy the current state to the mapped volume Output: Note: It also deleted the automated created private GitHub project. Destroy complete! Resources: 91 destroyed.","title":"Step 1 (inside the container): Destroy the created IBM Cloud resources"},{"location":"learn/iascable/lab3/#8-summary","text":"We achieved what we wanted to achieve, create a customized initial setup in an IBM Cloud environment for GitOps . The Technology Zone Accelerator Toolkit project and IasCable are powerful. As we have seen there was no need to write any Terraform module! Yes, when you are going to define you own \"Bill of Material BOM file\" you need to get familiar with the related modules related to your target architecture, when you want to customize it to your needs. But, as I said: There was no need to write own Terraform modules in our case. The TechZone Accelerator Toolkit project and IasCable project needs some more documentation in the future, I like the power of it and it is under Apache-2.0 license , which means you can use it as your starting point for Technology Zone Accelerator Toolkit with Terraform and contribute to the project.","title":"8. Summary"},{"location":"learn/iascable/lab4/","text":"Lab 4: Develop an own GitOps module \u00b6 1. Objective \u00b6 The objective is to understand how to build and use a custom module for the TechZone Accelerator Toolkit . Therefor a custom module will be created in GitOps scenario to deploy a helm-chart for an example application. The custom module will be deployed on a Red Hat OpenShift cluster on IBM Cloud with Argo CD configured for GitOps. 2. What do we cover in that lab 4? \u00b6 These are the major topics: We inspect the template-terraform-gitops . We create a custom module for Technology Zone Accelerator Toolkit step-by-step using the ubi-helm example from the Argo CD GitHub repository. We will see how to use a custom module in a BOM (Bill of material) We will see how to create and use a custom catalog for a custom module We will inspects the usage of a custom module That are the following sections: Understand the template-terraform-gitops Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example Implement the new terraform-gitops-ubi module Create an own catalog Create Terraform code with iascable and create the IBM Cloud resources with Terraform Verify the created Argo CD configuration on GitHub 3. Understand the template-terraform-gitops \u00b6 The template-terraform-gitops is a part of the How to instructions of the Technology Zone Accelerator Toolkit . The module covers the GitOps topic . 4. Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example \u00b6 These are the main tasks: Create a GitHub repository based on the gitops template from Software Everywhere Configure the terraform-gitops-ubi module Create an own catalog for the terraform-gitops-ubi module Create a BOM (Bill of material) where the terraform-gitops-ubi module is used and create the needed terraform output with iascable We will use later two catalogs and one BOM (Bill of material). here is a simplified view of the dependencies we will have later. 4.1 Prepare the environment \u00b6 4.1.1 Create a new GitHub repository based on the gitops template \u00b6 We clone the gitops template repository to our local computer and we going to create our terraform-gitops-ubi repository. Step 1: Clone the GitHub gitops template repository to your local computer and create a new GitHub repository based on that template \u00b6 You can follow the steps in the blog post to do this. Then you should have following folder structure on on computer: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 module.yaml \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 create-yaml.sh \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 stages \u2502 \u251c\u2500\u2500 stage0.tf \u2502 \u251c\u2500\u2500 stage1-cert.tf \u2502 \u251c\u2500\u2500 stage1-cluster.tf \u2502 \u251c\u2500\u2500 stage1-cp-catalogs.tf \u2502 \u251c\u2500\u2500 stage1-gitops-bootstrap.tf \u2502 \u251c\u2500\u2500 stage1-gitops.tf \u2502 \u251c\u2500\u2500 stage1-namespace.tf \u2502 \u251c\u2500\u2500 stage2-mymodule.tf \u2502 \u251c\u2500\u2500 stage3-outputs.tf \u2502 \u2514\u2500\u2500 variables.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf 4.1.2 Install iascable \u00b6 We install iascable to ensure you use the latest version. Step 1: Install iascable on your local computer \u00b6 curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh iascable --version Example output: 2 .17.4 4.1.2 Install a Multipass \u00b6 We will follow the instructions for Multipass . The following steps are an extractions of the cloud-native-toolkit documentation with small changes when needed. Step 1: Install Multipass with brew \u00b6 brew install --cask multipass Step 2: Download cloud-init configuration \u00b6 curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Step 3: Start the virtual cli-tools machine \u00b6 multipass launch --name cli-tools --cloud-init ./cli-tools.yaml 5. Implement the new terraform-gitops-ubi module \u00b6 In that section we will modify files in our newly created repository. These are the relevant files for our new module. The main.tf file The variable.tf file The helm chart content The module.yaml file Configure the helm chart copy automation in the scripts/create-yaml.sh file Create for terraform-gitops-ubi GitHub repository tags and releases 5.1 The main.tf file \u00b6 Step 1: Do some modifications in the main.tf file \u00b6 Change name = \"my-helm-chart-folder\" to ubi-helm First add ubi-helm = {// create entry} to the values_content = {} . That entry will be used to create the values for the variables in the values.yaml file for the helm chart. Below you see the relevant code in the main.tf which does the copy later. As you can is it uses the {local.name} value, so you need to ensure the name reflects the folder structure for your helm-chart later. resource null_resource create_yaml { provisioner \"local-exec\" { command = \" ${ path .module } /scripts/create-yaml.sh ' ${ local .name } ' ' ${ local .yaml_dir } '\" environment = { VALUES_CONTENT = yamlencode ( local.values_content ) } } } These are the values we need to insert for our terraform-gitops-ubi application as variables for the helm-chart. You find the variables in the Argo CD github project for the ubi-helm values.yaml Now replace the // create entry with the needed values. ubi-helm = { \"replicaCount\" : 1 \"image.repository\" = \"registry.access.redhat.com/ubi8/ubi\" } Change layer = \"services\" to layer = \"applications\" Add cluster_type = var.cluster_type == \"kubernetes\" ? \"kubernetes\" : \"openshift\" to the locals Resulting locals section in the main.tf file locals { na me = \"ubi-helm\" bi n _dir = module.se tu p_clis.bi n _dir yaml_dir = \"${path.cwd}/.tmp/${local.name}/chart/${local.name}\" service_url = \"http://${local.name}.${var.namespace}\" clus ter _ t ype = var.clus ter _ t ype == \"kubernetes\" ? \"kubernetes\" : \"openshift\" values_co ntent = { ubi - helm= { \"replicaCount\" : 1 \"image.repository\" = \"registry.access.redhat.com/ubi8/ubi\" \"image.tag\" = \"latest\" \"command\" = \"${var.command}\" } } layer = \"applications\" t ype = \"base\" applica t io n _bra n ch = \"main\" na mespace = var. na mespace layer_co nf ig = var.gi t ops_co nf ig [ local.layer ] } 5.2 The variable.tf file \u00b6 Step 1: Add some variables in the variable.tf file \u00b6 variable \"cluster_type\" { description = \"The cluster type (openshift or kubernetes)\" default = \"openshift\" } variable \"command\" { description = \"command to run in container\" default = \"\" } 5.3 The helm chart content \u00b6 Step 1: Create a new folder structure for the terraform-gitops-ubi helm chart \u00b6 Create following folder structure chart/ubi-helm . The name after chart must be the module name. \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 charts \u2502 \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u2502 \u2514\u2500\u2500 deployment.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 configmap.yaml \u2502 \u2502 \u251c\u2500\u2500 ubi-helm-v0.0.01.tgz \u2502 \u2502 \u2514\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 values.yaml That will be the resulting folder structure for the terraform-gitops-ubi module on your local pc: \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 chart \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 charts \u2502 \u2502 \u2514\u2500\u2500 ubi-helm \u2502 \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u2502 \u2514\u2500\u2500 deployment.yaml \u2502 \u2502 \u2502 \u2514\u2500\u2500 configmap.yaml \u2502 \u2502 \u251c\u2500\u2500 ubi-helm-v0.0.1.tgz \u2502 \u2502 \u2514\u2500\u2500 values.yaml \u2502 \u2514\u2500\u2500 values.yaml \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 module.yaml \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 create-yaml.sh \u251c\u2500\u2500 test \u2502 \u2514\u2500\u2500 stages \u2502 \u251c\u2500\u2500 stage0.tf \u2502 \u251c\u2500\u2500 stage1-cert.tf \u2502 \u251c\u2500\u2500 stage1-cluster.tf \u2502 \u251c\u2500\u2500 stage1-cp-catalogs.tf \u2502 \u251c\u2500\u2500 stage1-gitops-bootstrap.tf \u2502 \u251c\u2500\u2500 stage1-gitops.tf \u2502 \u251c\u2500\u2500 stage1-namespace.tf \u2502 \u251c\u2500\u2500 stage2-mymodule.tf \u2502 \u251c\u2500\u2500 stage3-outputs.tf \u2502 \u2514\u2500\u2500 variables.tf \u251c\u2500\u2500 variables.tf \u2514\u2500\u2500 version.tf Step 2: Copy in newly create folder structure the content from the repository for the ubi-helm chart https://github.com/ibm/ubi-helm/tree/main/charts/ubi-helm \u00b6 Step 3: Validate the helm chart with following commands: \u00b6 Navigate the charts directory CHARTDIR = ./chart/ubi-helm/charts/ubi-helm cd $CHARTDIR Verify the dependencies helm dep update . Verify the helm chart structure helm lint . Example output: == > Linting . [ INFO ] Chart.yaml: icon is recommended 1 chart ( s ) linted, 0 chart ( s ) failed helm template test . -n test Example output: # Source: ubi-helm/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: test-ubi-helm labels: app: ubi-helm chart: test-ubi-helm release: test heritage: Helm data: COMMAND: echo helloworld! --- # Source: ubi-helm/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: test-ubi-helm labels: app: ubi-helm chart: test-ubi-helm release: test heritage: Helm spec: replicas: 2 revisionHistoryLimit: 3 selector: matchLabels: app: ubi-helm release: test template: metadata: labels: app: ubi-helm release: test spec: containers: - name: ubi-helm image: \"registry.access.redhat.com/ubi8/ubi:latest\" imagePullPolicy: Always env: - name: COMMAND valueFrom: configMapKeyRef: name: test-ubi-helm key: COMMAND optional: true # mark the variable as optional command: [ \"/bin/sh\" , \"-c\" ] args: - while true ; do $COMMAND ; sleep 1000 ; done ; helm package . 5.4 The module.yaml file \u00b6 Step 1: Edited the module.yaml \u00b6 Use for name : terraform-gitops-ubi Use for description : That module will add a new Argo CD config to deploy the terraform-gitops-ubi application name : \"terraform-gitops-ubi\" type : gitops description : \"That module will add a new Argo CD config to deploy the terraform-gitops-ubi application\" tags : - tools - gitops versions : - platforms : - kubernetes - ocp3 - ocp4 dependencies : - id : gitops refs : - source : github.com/cloud-native-toolkit/terraform-tools-gitops.git version : \">= 1.1.0\" - id : namespace refs : - source : github.com/cloud-native-toolkit/terraform-gitops-namespace.git version : \">= 1.0.0\" variables : - name : gitops_config moduleRef : id : gitops output : gitops_config - name : git_credentials moduleRef : id : gitops output : git_credentials - name : server_name moduleRef : id : gitops output : server_name - name : namespace moduleRef : id : namespace output : name - name : kubeseal_cert moduleRef : id : gitops output : sealed_secrets_cert 5.5 Configure the helm chart copy automation in the scripts/create-yaml.sh file \u00b6 Step 1: Configure the scripts/create-yaml.sh in terraform-gitops-ubi repository \u00b6 Replace the existing code in scripts/create-yaml.sh with following content. This is important for later when the helm-chart will be copied. #!/usr/bin/env bash SCRIPT_DIR = $( cd $( dirname \" $0 \" ) ; pwd -P ) MODULE_DIR = $( cd \" ${ SCRIPT_DIR } /..\" ; pwd -P ) CHART_DIR = $( cd \" ${ SCRIPT_DIR } /../chart/ubi-helm\" ; pwd -P ) NAME = \" $1 \" DEST_DIR = \" $2 \" ## Add logic here to put the yaml resource content in DEST_DIR mkdir -p \" ${ DEST_DIR } \" cp -R \" ${ CHART_DIR } /\" * \" ${ DEST_DIR } \" if [[ -n \" ${ VALUES_CONTENT } \" ]] ; then echo \" ${ VALUES_CONTENT } \" > \" ${ DEST_DIR } /values.yaml\" fi find \" ${ DEST_DIR } \" -name \"*\" echo \"Files in output path\" ls -l \" ${ DEST_DIR } \" 5.6 terraform-gitops-ubi GitHub repository tags and releases \u00b6 The release tag represents the version number of our module. terraform-gitops-ubi Step 1: Create GitHub tag and release for the terraform-gitops-ubi GitHub repository \u00b6 The module github repository release tags should be updated when you are going to change the terraform-gitops-ubi GitHub repository module. The image below shows some releases and as you can see for each release an archive is available. Later iascable uses the release tag to download the right archive to the local computer to create the Terraform output. In case when you use specific version numbers in the BOM which uses the module, you need to ensure that version number is also in range of the custom chart which points to the module. That is also relevant for the catalog.yaml we will define later. Example relevant extract from a BOM -> version: v0.0.7 # Install terraform-gitops-ubi # New custom module linked be the custom catalog - name : terraform-gitops-ubi version : v0.0.8 variables : - name : command value : \"echo 'helloworld'\" You can follow the step to create a GitHub tag is that example blog post and then create a release. 6. Create an own catalog \u00b6 In that example we will not publish the our terraform-gitops-ubi module to the public catalog on Technology Zone Accelerator Toolkit . We will create our own catalog.yaml file and save the configuration in the GitHub project of the module. How to create catalog.yaml file? How to combine various catalogs? Inspect the structure of a catalog.yaml Create a custom catalog steps The following diagram shows the simplified dependencies of module , catalog and iascable : 6.1 How to create catalog.yaml file? \u00b6 It is useful to take a look into iascable documentation and the build-catalog.sh automation . 6.2 How to combine various catalogs? \u00b6 You can combine more than one catalog resources and BOM inputs with the iascable build command. Here is the build command: iascable build [ -c { CATALOG_URL }] [ -c { CATALOG_URL }] -i { BOM_INPUT } [ -i { BOM_INPUT }] [ -o { OUTPUT_DIR }] CATALOG_URL is the url of the module catalog. The default module catalog is https://modules.cloudnativetoolkit.dev/index.yaml. Multiple module catalogs can be provided. The catalogs are combined, with the last one taking precedence in the case of duplicate modules. BOM_INPUT is the input file containing the Bill of Material definition. Multiple BOM files can be provided at the same time. OUTPUT_DIR is the directory where the output terraform template will be generated. 6.3 Inspect the structure of a catalog.yaml \u00b6 The structure of a catalog can be verified here https://modules.cloudnativetoolkit.dev/index.yaml That is a minimize extraction of the index.yaml above. It contains: categories , modules , aliases and providers . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : ai-ml - category : cluster - category : databases - category : dev-tool - category : gitops categoryName : GitOps selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops name : gitops-ocs-operator description : Module to populate a gitops repo with the resources to provision ocs-operator tags : - tools - gitops versions : [] id : github.com/cloud-native-toolkit/terraform-gitops-ocs-operator group : \"\" displayName : ocs-operator - category : iam - category : image-registry - category : infrastructure ... aliases : - id : github.com/terraform-ibm-modules/terraform-ibm-toolkit-mongodb ... providers : - name : ibm source : ibm-cloud/ibm variables : - name : ibmcloud_api_key scope : global - name : region scope : global 6.4 Inspect the module section of the catalog file in more detail \u00b6 We see that the modules section does contain following cloudProvider , softwareProvider , id , group , displayName and type which are not a part of the module.yaml . After these entries we insert content of the module.yaml . Current gitops template . 6.5 Create a custom catalog \u00b6 Step 1: Create a terraform-gitops-ubi-catalog.yml and insert following content \u00b6 Note: Ensure that the github project has a tag and a release! The right value of the release must be reference in the catalog! (Example version: v0.0.1 ). apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : examples categoryName : examples selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops id : github.com/Vishal-Ramani/terraform-gitops-ubi group : \"\" displayName : terraform-gitops-ubi name : terraform-gitops-ubi description : \"That module will add a new 'Argo CD config' to deploy a 'ubi' container to OpenShift\" tags : - tools - gitops versions : - platforms : - kubernetes - ocp3 - ocp4 version : v0.0.8 dependencies : - id : gitops refs : - source : github.com/cloud-native-toolkit/terraform-tools-gitops.git version : '>= 1.1.0' - id : namespace refs : - source : github.com/cloud-native-toolkit/terraform-gitops-namespace.git version : '>= 1.0.0' variables : - name : gitops_config type : |- object({ boostrap = object({ argocd-config = object({ project = string repo = string url = string path = string }) }) infrastructure = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) services = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) applications = object({ argocd-config = object({ project = string repo = string url = string path = string }) payload = object({ repo = string url = string path = string }) }) }) description : Config information regarding the gitops repo structure moduleRef : id : gitops output : gitops_config - name : git_credentials type : |- list(object({ repo = string url = string username = string token = string })) description : The credentials for the gitops repo(s) sensitive : true moduleRef : id : gitops output : git_credentials - name : namespace type : string description : The namespace where the application should be deployed moduleRef : id : namespace output : name - name : kubeseal_cert type : string description : The certificate/public key used to encrypt the sealed secrets default : \"\" moduleRef : id : gitops output : sealed_secrets_cert - name : server_name type : string description : The name of the server default : default moduleRef : id : gitops output : server_name - name : cluster_type description : The cluster type (openshift or kubernetes) default : '\"openshift\"' - name : command description : Command to run in container default : \"\" outputs : - name : name description : The name of the module - name : branch description : The branch where the module config has been placed - name : namespace description : The namespace where the module will be deployed - name : server_name description : The server where the module will be deployed - name : layer description : The layer where the module is deployed - name : type description : The type of module where the module is deployed 6.6. BOM that we will use terraform-gitops-ubi module \u00b6 Step 1: Clone the project with the example BOM configuration \u00b6 git clone https://github.com/Vishal-Ramani/UBI-helm-module-example.git cd example Step 2: Verify the ibm-vpc-roks-argocd-ubi.yaml BOM file \u00b6 This is the simplified target architecture what our BOM will create as terraform code for initial setup. A customized IBM Cloud environment for GitOps and our terraform-gitops-ubi module. For the configuration of GitOps in Red Hat OpenShift. We will use two operators: Red Hat OpenShift GitOps operator We will create one ArgoCD instance with the Red at OpenShift GitOps operator, that ArgoCD instance will bin initial configured by a newly created GitHub project configure by a Cloud Native Toolkit template for GitOps repositories. Red Hat OpenShift Pipelines operator There will be no initial setup for a Tekton pipeline at the moment. That images show a simplified view of the Argo CD basic configuration. IBM Cloud infrastructure with Red Hat OpenShift in a Virtual Private Cloud This is the structure of the BOM we are going to use: Virtual Private Cloud - related ROKS - related (RedHat OpenShift on IBM Cloud) GitOps and Bootstrap of GitOps Our own module called terraform-gitops-ubi Note: You need configure variables to your needs, when you share your IBM Cloud environment with others. We commented out the # version: v0.0.5 of our module, because we will configure only one version in our catalog.yaml which we will define later. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : ibm-vpc-roks-argocd-ubi spec : modules : # Virtual Private Cloud - related # - subnets # - gateways - name : ibm-vpc alias : ibm-vpc version : v1.16.1 variables : - name : name value : \"tsued-gitops-ubi\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-subnets alias : ibm-vpc-subnets version : v1.13.2 variables : - name : _count value : 1 - name : name value : \"tsued-gitops-ubi\" - name : tags value : [ \"tsuedro\" ] - name : ibm-vpc-gateways # ROKS - related # - objectstorage - name : ibm-ocp-vpc alias : ibm-ocp-vpc version : v1.15.7 variables : - name : name value : \"tsued-gitops-ubi\" - name : worker_count value : 2 - name : tags value : [ \"tsuedro\" ] - name : ibm-object-storage alias : ibm-object-storage version : v4.0.3 variables : - name : name value : \"cos_tsued_ubi\" - name : tags value : [ \"tsuedro\" ] - name : label value : [ \"cos_tsued_ubi\" ] # Install OpenShift GitOps and Bootstrap GitOps (aka. ArgoCD) - related # - argocd # - gitops - name : argocd-bootstrap alias : argocd-bootstrap version : v1.12.0 variables : - name : repo_token - name : gitops-repo alias : gitops-repo version : v1.20.2 variables : - name : host value : \"github.com\" - name : type value : \"GIT\" - name : org value : \"thomassuedbroecker\" - name : username value : \"thomassuedbroecker\" - name : project value : \"iascable-gitops-ubi\" - name : repo value : \"iascable-gitops-ubi\" # Install ubi # New custom module linked be the custom catalog - name : terraform-gitops-ubi version : v0.0.8 variables : - name : command value : \"echo 'helloworld'\" 7. Create terraform code and create the resources \u00b6 Use iascable to create the terraform code. Step 1: Create a credentials.properties file and edit the file \u00b6 cd example cp ./credentials.properties-template ./credentials.properties nano credentials.properties Provide the your GitHub access token and IBM Cloud API key. export TF_VAR_gitops_repo_token = XXX export TF_VAR_ibmcloud_api_key = XXX Step 2: Execute following commands \u00b6 BASE_CATALOG = https://modules.cloudnativetoolkit.dev/index.yaml CUSTOM_CATALOG = https://raw.githubusercontent.com/Vishal-Ramani/UBI-helm-module-example/main/example/catalog/ubi-helm-catalog.yaml iascable build -i ibm-vpc-roks-argocd-ubi.yaml -c $BASE_CATALOG -c $CUSTOM_CATALOG Step 3: Verify the created files and folders \u00b6 tree . Example output: . \u251c\u2500\u2500 catalog \u2502 \u2514\u2500\u2500 ubi-helm-catalog.yaml \u251c\u2500\u2500 credentials.properties \u251c\u2500\u2500 credentials.properties-template \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 argocd-bootstrap.md \u2502 \u2502 \u251c\u2500\u2500 gitops-namespace.md \u2502 \u2502 \u251c\u2500\u2500 gitops-repo.md \u2502 \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc.md \u2502 \u2502 \u251c\u2500\u2500 olm.md \u2502 \u2502 \u251c\u2500\u2500 sealed-secret-cert.md \u2502 \u2502 \u2514\u2500\u2500 terraform-gitops-ubi.md \u2502 \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh 5 directories, 26 files Step 4: Navigate to the output folder \u00b6 cd output Step 5: Copy the credentials.properties into the output folder \u00b6 CURRENT_PATH = $( pwd ) PROJECT = ibm-vpc-roks-argocd-ubi cp $CURRENT_PATH /../credentials.properties $CURRENT_PATH /ibm-vpc-roks-argocd-ubi/credentials.properties Step 6: Map the current folder to the Multipass cli-tools VM \u00b6 Ensure you started the Multipass cli-tools VM before you execute the following command: multipass mount $PWD cli-tools:/automation Now we have mapped the output folder to the cli-tools VM . We can use the installed CLI tools inside the cli-tools VM to apply the Terraform code. All changes we made in with cli-tools VM will be saved in the mapped output folder on our local machine. Step 7: Open the interactive shell \u00b6 multipass shell cli-tools Example output: Last login: Mon Sep 12 18 :06:24 2022 from 192 .168.64.1 ubuntu@cli-tools:~$ Step 8: In the virtual machine navigate to the automation folder \u00b6 cd ../../automation ls Step 9: Now navigate to the ibm-vpc-roks-argocd-ubi folder \u00b6 cd ibm-vpc-roks-argocd-ubi/ ls Step 10: Source the credentials.properties as environment variables and show one variable \u00b6 source credentials.properties echo $TF_VAR_ibmcloud_api_key Step 11: Execute ./apply.sh \u00b6 ./apply.sh Step 12: Enter yes to apply the Terraform code \u00b6 Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: Step 13: Interactive terminal actions \u00b6 These values you need to edit: Namespace: ubi-helm Region: eu-de Resource group: default gitops-repo_token: XXX Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops-ubi ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXXX > Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'common_tags' : Common tags that should be added to the instance > ([]) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Provide a value for 'namespace_name' : The value that should be used for the namespace > ubi-helm Step 14: Verify the output of terraform execution \u00b6 After some time you should get following output: Apply complete! Resources: 103 added, 0 changed, 0 destroyed. Step 15: Open Argo CD in OpenShift and verify the application instances \u00b6 Follow the steps in the shown in the gif . Step 16: Access the UBI pod in OpenShift and execute ls in the terminal \u00b6 Follow the steps in the shown in the gif . 8. Verify the created Argo CD configuration on GitHub \u00b6 We see that in our GitHub account new repository was created from the GitOps bootstrap module and the terraform-tools-gitops module to figure Argo CD for by using the app-of-apps concept with a single GitHub repository to manage all Argo CD application configuration and helm configurations to deploy applications in the GitOps context. Reminder the boot strap configuration is shown in the following image for details visit the GitOps Repository Structure module. The new GitHub repository is called iascable-gitops-ubi in our case. The new iascable-gitops-ubi repository contains two folders the following image shows the relation to the bootstrap configuration. argocd folder which contains the configuration for Argo CD let us call it app-of-apps folder. The following image displays the resulting configuration in Argo CD payload folder which contains the current helm deployment for the apps which will be deployed. The following image show the deployment created by apps in our case the ubi-helm. The following image shows the newly created GitHub iascable-gitops-ubi repository. For more details visit the template of the terraform-tools-gitops module. 8.1 Understand how the ubi module content was pasted into the new iascable-gitops-ubi repository \u00b6 Following the concept for the gitops bootstrap setup documented in the template-terraform-gitops GitHub repository. We have two main folders in the iascable-gitops-ubi repository. One for the Argo CD application configurations called argocd One for the application which will be deployed be the Argo CD application configurations called payload. Let us inspect these two folders. The gif below shows some of the created files and folders. 8.1.1 argocd folder \u00b6 There were two Argo CD application configurations added into the iascable-gitops-ubi repository. One for the namespace in the OpenShift or Kubernetes cluster where the ubi application will be deployed. That Argo CD application configuration is related to exiting 1-infrastructure Argo CD project created by the GitOps bootstrap module . One for the ubi application we want to deploy. That Argo CD application configuration is related to exiting 3-application Argo CD project created by the GitOps bootstrap module . Let's take a look a the created Argo CD application configurations We have two Argo CD application configurations: 8.1.1.1 ubi Namespace in argocd.1-infrastructure.cluster.default.base.namespace-ubi-helm.yaml \u00b6 apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : namespace-ubi-helm finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : default server : https://kubernetes.default.svc project : 1-infrastructure source : path : payload/1-infrastructure/namespace/ubi-helm/namespace repoURL : https://github.com/thomassuedbroecker/iascable-gitops-ubi.git targetRevision : main syncPolicy : automated : prune : true selfHeal : true ignoreDifferences : [] 8.1.1.2 UBI application deployment argocd.3-applications.cluster.default.base.ubi-helm-ubi.yaml \u00b6 This is the Argo CD application configuration ubi-helm-ubi-helm.yaml file, which was created automatically by our module with the igc gitops-module command. That payload directory is used as the source.path in that Argo CD application configuration as you see above. apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : ubi-helm-ubi-helm finalizers : - resources-finalizer.argocd.argoproj.io spec : destination : namespace : ubi-helm server : https://kubernetes.default.svc project : 3-applications source : path : payload/3-applications/namespace/ubi-helm/ubi-helm repoURL : https://github.com/thomassuedbroecker/iascable-gitops-ubi.git targetRevision : main helm : releaseName : ubi-helm syncPolicy : automated : prune : true selfHeal : true ignoreDifferences : [] 8.1.2 payload folder \u00b6 That folder contains a namespace payload and the helm-chart payload. 8.1.2.2 ubi Namespace in payload.1-infrastructure.cluster.default.base \u00b6 In the folder payload.1-infrastructure.cluster.default.base we have an ns.yaml and rbac.yaml . ns.yaml apiVersion : v1 kind : Namespace metadata : name : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-30\" --- apiVersion : operators.coreos.com/v1 kind : OperatorGroup metadata : name : ubi-helm-operator-group namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" spec : targetNamespaces : - ubi-helm rbac.yaml apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : argocd-admin namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" rules : - apiGroups : - \"*\" resources : - \"*\" verbs : - \"*\" --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : argocd-admin namespace : ubi-helm annotations : argocd.argoproj.io/sync-wave : \"-20\" roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : argocd-admin subjects : - apiGroup : rbac.authorization.k8s.io kind : Group name : system:serviceaccounts:openshift-gitops 8.1.1.2 ubi helm application deployment payload.3-applications.cluster.default.base \u00b6 That folder contains the ubi application helm chart configuration to deploy the ubi application. The script scripts/create-yaml.sh of our module terraform-gitops-ubi was responsible to copy the ubi helm-chart into the payload directory. Therefor we did the customization of that file. We defined the values content for the helm chart variables before in the module.tf file. That file values.yaml file is used in Argo CD application configuration for the parameters. values_content = { ubi-helm = { // create entry } } The following gif shows the relation of the parameter configuration for the helm-chart","title":"Lab 4: Develop GitOps module"},{"location":"learn/iascable/lab4/#lab-4-develop-an-own-gitops-module","text":"","title":"Lab 4: Develop an own GitOps module"},{"location":"learn/iascable/lab4/#1-objective","text":"The objective is to understand how to build and use a custom module for the TechZone Accelerator Toolkit . Therefor a custom module will be created in GitOps scenario to deploy a helm-chart for an example application. The custom module will be deployed on a Red Hat OpenShift cluster on IBM Cloud with Argo CD configured for GitOps.","title":"1. Objective"},{"location":"learn/iascable/lab4/#2-what-do-we-cover-in-that-lab-4","text":"These are the major topics: We inspect the template-terraform-gitops . We create a custom module for Technology Zone Accelerator Toolkit step-by-step using the ubi-helm example from the Argo CD GitHub repository. We will see how to use a custom module in a BOM (Bill of material) We will see how to create and use a custom catalog for a custom module We will inspects the usage of a custom module That are the following sections: Understand the template-terraform-gitops Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example Implement the new terraform-gitops-ubi module Create an own catalog Create Terraform code with iascable and create the IBM Cloud resources with Terraform Verify the created Argo CD configuration on GitHub","title":"2. What do we cover in that lab 4?"},{"location":"learn/iascable/lab4/#3-understand-the-template-terraform-gitops","text":"The template-terraform-gitops is a part of the How to instructions of the Technology Zone Accelerator Toolkit . The module covers the GitOps topic .","title":"3. Understand the template-terraform-gitops"},{"location":"learn/iascable/lab4/#4-use-the-template-terraform-gitops-to-create-a-module-to-deploy-the-terraform-gitops-ubi-example","text":"These are the main tasks: Create a GitHub repository based on the gitops template from Software Everywhere Configure the terraform-gitops-ubi module Create an own catalog for the terraform-gitops-ubi module Create a BOM (Bill of material) where the terraform-gitops-ubi module is used and create the needed terraform output with iascable We will use later two catalogs and one BOM (Bill of material). here is a simplified view of the dependencies we will have later.","title":"4. Use the template-terraform-gitops to create a module to deploy the terraform-gitops-ubi example"},{"location":"learn/iascable/lab4/#41-prepare-the-environment","text":"","title":"4.1 Prepare the environment"},{"location":"learn/iascable/lab4/#411-create-a-new-github-repository-based-on-the-gitops-template","text":"We clone the gitops template repository to our local computer and we going to create our terraform-gitops-ubi repository.","title":"4.1.1 Create a new GitHub repository based on the gitops template"},{"location":"learn/iascable/lab4/#412-install-iascable","text":"We install iascable to ensure you use the latest version.","title":"4.1.2 Install iascable"},{"location":"learn/iascable/lab4/#412-install-a-multipass","text":"We will follow the instructions for Multipass . The following steps are an extractions of the cloud-native-toolkit documentation with small changes when needed.","title":"4.1.2 Install a Multipass"},{"location":"learn/iascable/lab4/#step-2-download-cloud-init-configuration","text":"curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml","title":"Step 2: Download cloud-init configuration"},{"location":"learn/iascable/lab4/#step-3-start-the-virtual-cli-tools-machine","text":"multipass launch --name cli-tools --cloud-init ./cli-tools.yaml","title":"Step 3: Start the virtual cli-tools machine"},{"location":"learn/iascable/lab4/#5-implement-the-new-terraform-gitops-ubi-module","text":"In that section we will modify files in our newly created repository. These are the relevant files for our new module. The main.tf file The variable.tf file The helm chart content The module.yaml file Configure the helm chart copy automation in the scripts/create-yaml.sh file Create for terraform-gitops-ubi GitHub repository tags and releases","title":"5. Implement the new terraform-gitops-ubi module"},{"location":"learn/iascable/lab4/#51-the-maintf-file","text":"","title":"5.1 The main.tf file"},{"location":"learn/iascable/lab4/#52-the-variabletf-file","text":"","title":"5.2 The variable.tf file"},{"location":"learn/iascable/lab4/#53-the-helm-chart-content","text":"","title":"5.3 The helm chart content"},{"location":"learn/iascable/lab4/#54-the-moduleyaml-file","text":"","title":"5.4 The module.yaml file"},{"location":"learn/iascable/lab4/#55-configure-the-helm-chart-copy-automation-in-the-scriptscreate-yamlsh-file","text":"","title":"5.5 Configure the helm chart copy automation in the scripts/create-yaml.sh file"},{"location":"learn/iascable/lab4/#56-terraform-gitops-ubi-github-repository-tags-and-releases","text":"The release tag represents the version number of our module. terraform-gitops-ubi","title":"5.6 terraform-gitops-ubi GitHub repository tags and releases"},{"location":"learn/iascable/lab4/#6-create-an-own-catalog","text":"In that example we will not publish the our terraform-gitops-ubi module to the public catalog on Technology Zone Accelerator Toolkit . We will create our own catalog.yaml file and save the configuration in the GitHub project of the module. How to create catalog.yaml file? How to combine various catalogs? Inspect the structure of a catalog.yaml Create a custom catalog steps The following diagram shows the simplified dependencies of module , catalog and iascable :","title":"6. Create an own catalog"},{"location":"learn/iascable/lab4/#61-how-to-create-catalogyaml-file","text":"It is useful to take a look into iascable documentation and the build-catalog.sh automation .","title":"6.1 How to create catalog.yaml file?"},{"location":"learn/iascable/lab4/#62-how-to-combine-various-catalogs","text":"You can combine more than one catalog resources and BOM inputs with the iascable build command. Here is the build command: iascable build [ -c { CATALOG_URL }] [ -c { CATALOG_URL }] -i { BOM_INPUT } [ -i { BOM_INPUT }] [ -o { OUTPUT_DIR }] CATALOG_URL is the url of the module catalog. The default module catalog is https://modules.cloudnativetoolkit.dev/index.yaml. Multiple module catalogs can be provided. The catalogs are combined, with the last one taking precedence in the case of duplicate modules. BOM_INPUT is the input file containing the Bill of Material definition. Multiple BOM files can be provided at the same time. OUTPUT_DIR is the directory where the output terraform template will be generated.","title":"6.2 How to combine various catalogs?"},{"location":"learn/iascable/lab4/#63-inspect-the-structure-of-a-catalogyaml","text":"The structure of a catalog can be verified here https://modules.cloudnativetoolkit.dev/index.yaml That is a minimize extraction of the index.yaml above. It contains: categories , modules , aliases and providers . apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : Catalog categories : - category : ai-ml - category : cluster - category : databases - category : dev-tool - category : gitops categoryName : GitOps selection : multiple modules : - cloudProvider : \"\" softwareProvider : \"\" type : gitops name : gitops-ocs-operator description : Module to populate a gitops repo with the resources to provision ocs-operator tags : - tools - gitops versions : [] id : github.com/cloud-native-toolkit/terraform-gitops-ocs-operator group : \"\" displayName : ocs-operator - category : iam - category : image-registry - category : infrastructure ... aliases : - id : github.com/terraform-ibm-modules/terraform-ibm-toolkit-mongodb ... providers : - name : ibm source : ibm-cloud/ibm variables : - name : ibmcloud_api_key scope : global - name : region scope : global","title":"6.3 Inspect the structure of a catalog.yaml"},{"location":"learn/iascable/lab4/#64-inspect-the-module-section-of-the-catalog-file-in-more-detail","text":"We see that the modules section does contain following cloudProvider , softwareProvider , id , group , displayName and type which are not a part of the module.yaml . After these entries we insert content of the module.yaml . Current gitops template .","title":"6.4 Inspect the module section of the catalog file in more detail"},{"location":"learn/iascable/lab4/#65-create-a-custom-catalog","text":"","title":"6.5 Create a custom catalog"},{"location":"learn/iascable/lab4/#66-bom-that-we-will-use-terraform-gitops-ubi-module","text":"","title":"6.6. BOM that we will use terraform-gitops-ubi module"},{"location":"learn/iascable/lab4/#7-create-terraform-code-and-create-the-resources","text":"Use iascable to create the terraform code.","title":"7. Create terraform code and create the resources"},{"location":"learn/iascable/lab4/#step-1-create-a-credentialsproperties-file-and-edit-the-file","text":"cd example cp ./credentials.properties-template ./credentials.properties nano credentials.properties Provide the your GitHub access token and IBM Cloud API key. export TF_VAR_gitops_repo_token = XXX export TF_VAR_ibmcloud_api_key = XXX","title":"Step 1: Create a credentials.properties file and edit the file"},{"location":"learn/iascable/lab4/#step-2-execute-following-commands","text":"BASE_CATALOG = https://modules.cloudnativetoolkit.dev/index.yaml CUSTOM_CATALOG = https://raw.githubusercontent.com/Vishal-Ramani/UBI-helm-module-example/main/example/catalog/ubi-helm-catalog.yaml iascable build -i ibm-vpc-roks-argocd-ubi.yaml -c $BASE_CATALOG -c $CUSTOM_CATALOG","title":"Step 2: Execute following commands"},{"location":"learn/iascable/lab4/#step-3-verify-the-created-files-and-folders","text":"tree . Example output: . \u251c\u2500\u2500 catalog \u2502 \u2514\u2500\u2500 ubi-helm-catalog.yaml \u251c\u2500\u2500 credentials.properties \u251c\u2500\u2500 credentials.properties-template \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.yaml \u2514\u2500\u2500 output \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi \u2502 \u251c\u2500\u2500 apply.sh \u2502 \u251c\u2500\u2500 bom.yaml \u2502 \u251c\u2500\u2500 dependencies.dot \u2502 \u251c\u2500\u2500 destroy.sh \u2502 \u2514\u2500\u2500 terraform \u2502 \u251c\u2500\u2500 docs \u2502 \u2502 \u251c\u2500\u2500 argocd-bootstrap.md \u2502 \u2502 \u251c\u2500\u2500 gitops-namespace.md \u2502 \u2502 \u251c\u2500\u2500 gitops-repo.md \u2502 \u2502 \u251c\u2500\u2500 ibm-object-storage.md \u2502 \u2502 \u251c\u2500\u2500 ibm-ocp-vpc.md \u2502 \u2502 \u251c\u2500\u2500 ibm-resource-group.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-gateways.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc-subnets.md \u2502 \u2502 \u251c\u2500\u2500 ibm-vpc.md \u2502 \u2502 \u251c\u2500\u2500 olm.md \u2502 \u2502 \u251c\u2500\u2500 sealed-secret-cert.md \u2502 \u2502 \u2514\u2500\u2500 terraform-gitops-ubi.md \u2502 \u251c\u2500\u2500 ibm-vpc-roks-argocd-ubi.auto.tfvars \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 version.tf \u2514\u2500\u2500 launch.sh 5 directories, 26 files","title":"Step 3: Verify the created files and folders"},{"location":"learn/iascable/lab4/#step-4-navigate-to-the-output-folder","text":"cd output","title":"Step 4: Navigate to the output folder"},{"location":"learn/iascable/lab4/#step-5-copy-the-credentialsproperties-into-the-output-folder","text":"CURRENT_PATH = $( pwd ) PROJECT = ibm-vpc-roks-argocd-ubi cp $CURRENT_PATH /../credentials.properties $CURRENT_PATH /ibm-vpc-roks-argocd-ubi/credentials.properties","title":"Step 5: Copy the credentials.properties into the output folder"},{"location":"learn/iascable/lab4/#step-6-map-the-current-folder-to-the-multipass-cli-tools-vm","text":"Ensure you started the Multipass cli-tools VM before you execute the following command: multipass mount $PWD cli-tools:/automation Now we have mapped the output folder to the cli-tools VM . We can use the installed CLI tools inside the cli-tools VM to apply the Terraform code. All changes we made in with cli-tools VM will be saved in the mapped output folder on our local machine.","title":"Step 6: Map the current folder to the Multipass cli-tools VM"},{"location":"learn/iascable/lab4/#step-7-open-the-interactive-shell","text":"multipass shell cli-tools Example output: Last login: Mon Sep 12 18 :06:24 2022 from 192 .168.64.1 ubuntu@cli-tools:~$","title":"Step 7: Open the interactive shell"},{"location":"learn/iascable/lab4/#step-8-in-the-virtual-machine-navigate-to-the-automation-folder","text":"cd ../../automation ls","title":"Step 8: In the virtual machine navigate to the automation folder"},{"location":"learn/iascable/lab4/#step-9-now-navigate-to-the-ibm-vpc-roks-argocd-ubi-folder","text":"cd ibm-vpc-roks-argocd-ubi/ ls","title":"Step 9: Now navigate to the ibm-vpc-roks-argocd-ubi folder"},{"location":"learn/iascable/lab4/#step-10-source-the-credentialsproperties-as-environment-variables-and-show-one-variable","text":"source credentials.properties echo $TF_VAR_ibmcloud_api_key","title":"Step 10: Source the credentials.properties as environment variables and show one variable"},{"location":"learn/iascable/lab4/#step-11-execute-applysh","text":"./apply.sh","title":"Step 11: Execute ./apply.sh"},{"location":"learn/iascable/lab4/#step-12-enter-yes-to-apply-the-terraform-code","text":"Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value:","title":"Step 12: Enter yes to apply the Terraform code"},{"location":"learn/iascable/lab4/#step-13-interactive-terminal-actions","text":"These values you need to edit: Namespace: ubi-helm Region: eu-de Resource group: default gitops-repo_token: XXX Variables can be provided in a yaml file passed as the first argument Provide a value for 'gitops-repo_host' : The host for the git repository. The git host used can be a GitHub, GitHub Enterprise, Gitlab, Bitbucket, Gitea or Azure DevOps server. If the host is null assumes in -cluster Gitea instance will be used. > ( github.com ) Provide a value for 'gitops-repo_org' : The org/group where the git repository exists/will be provisioned. If the value is left blank then the username org will be used. > ( thomassuedbroecker ) Provide a value for 'gitops-repo_project' : The project that will be used for the git repo. ( Primarily used for Azure DevOps repos ) > ( iascable-gitops-ubi ) Provide a value for 'gitops-repo_username' : The username of the user with access to the repository > ( thomassuedbroecker ) Provide a value for 'gitops-repo_token' : The personal access token used to access the repository > XXXX > Provide a value for 'region' : > eu-de Provide a value for 'worker_count' : The number of worker nodes that should be provisioned for classic infrastructure > ( 2 ) Provide a value for 'ibm-ocp-vpc_flavor' : The machine type that will be provisioned for classic infrastructure > ( bx2.4x16 ) Provide a value for 'common_tags' : Common tags that should be added to the instance > ([]) Provide a value for 'ibm-vpc-subnets__count' : The number of subnets that should be provisioned > ( 1 ) Provide a value for 'resource_group_name' : The name of the resource group > default Provide a value for 'namespace_name' : The value that should be used for the namespace > ubi-helm","title":"Step 13: Interactive terminal actions"},{"location":"learn/iascable/lab4/#8-verify-the-created-argo-cd-configuration-on-github","text":"We see that in our GitHub account new repository was created from the GitOps bootstrap module and the terraform-tools-gitops module to figure Argo CD for by using the app-of-apps concept with a single GitHub repository to manage all Argo CD application configuration and helm configurations to deploy applications in the GitOps context. Reminder the boot strap configuration is shown in the following image for details visit the GitOps Repository Structure module. The new GitHub repository is called iascable-gitops-ubi in our case. The new iascable-gitops-ubi repository contains two folders the following image shows the relation to the bootstrap configuration. argocd folder which contains the configuration for Argo CD let us call it app-of-apps folder. The following image displays the resulting configuration in Argo CD payload folder which contains the current helm deployment for the apps which will be deployed. The following image show the deployment created by apps in our case the ubi-helm. The following image shows the newly created GitHub iascable-gitops-ubi repository. For more details visit the template of the terraform-tools-gitops module.","title":"8. Verify the created Argo CD configuration on GitHub"},{"location":"learn/iascable/lab4/#81-understand-how-the-ubi-module-content-was-pasted-into-the-new-iascable-gitops-ubi-repository","text":"Following the concept for the gitops bootstrap setup documented in the template-terraform-gitops GitHub repository. We have two main folders in the iascable-gitops-ubi repository. One for the Argo CD application configurations called argocd One for the application which will be deployed be the Argo CD application configurations called payload. Let us inspect these two folders. The gif below shows some of the created files and folders.","title":"8.1 Understand how the ubi module content was pasted into the new iascable-gitops-ubi repository"},{"location":"reference/","text":"Reference \u00b6 Todo Complete this section Reference material for Module directory template content Module file syntax BOM syntax CLI parameters (iascable) Links to useful resources","title":"Overview"},{"location":"reference/#reference","text":"Todo Complete this section Reference material for Module directory template content Module file syntax BOM syntax CLI parameters (iascable) Links to useful resources","title":"Reference"},{"location":"reference/bom/","text":"Bill of Material Reference \u00b6 Todo Complete this reference section check the content as moved from previous documentation structure. Bill of Material reference \u00b6 The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/ BOM metadata \u00b6 The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform BOM spec \u00b6 The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations BOM module definition \u00b6 A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules. BOM Module dependencies \u00b6 If the module depends on other modules, the relationships can be defined in the dependencies block. However, in most cases it is not necessary to explicitly define the dependencies. Through the module metadata, the iascable tool knows the required dependencies for each module and can \"auto-wire\" the modules together. If necessary, iascable will automatically add modules to the BOM if they are required to satisfy a required module dependency. If there are multiple instances of a dependent module defined in the BOM then iascable will \"auto-wire\" the dependency to the \"default\" dependent module. The \"default\" dependent module is the one that uses the default alias name OR has the default: true attribute added to it. If a default cannot be identified then ANOTHER instance of the module will be automatically added to the BOM. If this behavior is not desired then the desired dependent module can be referenced in the dependencies block. For example: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp The ibm-vpc-subnets module depends on ibm-vpc . An explicit declaration of the dependency is not required here though because the ibm-vpc module is the default instance and all of the ibm-vpc-subnets are auto-wired to that instance. (In fact the ibm-vpc module doesn't even need to be explicitly listed in the BOM in this case, but it is added for completeness.) The ibm-vpc-ocp module depends on ibm-vpc-subnets to identify where the cluster should be deployed. In this configuration, a default ibm-vpc-subnets instance has not been defined. As a result, iascable will automatically pull in 4th ibm-vpc-subnets instance to satisfy the dependency. This is probably not the desired result and we will want to explicitly define the dependency in the BOM. The updated BOM would look like the following: spec : modules : - name : ibm-vpc - name : ibm-vpc-subnets alias : edge_subnets - name : ibm-vpc-subnets alias : cluster_subnets - name : ibm-vpc-subnets alias : vpe_subnets - name : ibm-vpc-ocp dependencies : - id : subnets ref : cluster_subnets The subnets identifier in the dependencies array refers to the dependency identifier in the module metadata for the ibm-vpc-ocp module. The cluster_subnets value refers to the alias of the target ibm-vpc-subnets module instance. Note: The only exception to iascable automatically pulling dependent modules into the BOM is if there are multiple module options that satisfy the dependency. In this case one of the modules that satisfies dependency must be explicitly added to the BOM. Otherwise the iascable build command will give an error that the dependency cannot be resolved. Todo Should there be an example to show the use of the default flag? BOM Module variables \u00b6 The Bill of Materials also allows the module variables to be configured in a variables block. The variables block is an array of variable definitions. At a minimum the variable name must be provided. The available variable names are defined in the module metadata. For each variable, the following values can be provided: Field Description value The default value of the variable. This value will override the default in the module. scope The scope of the variable that defines how the variable will be handled in the global variable namespace. Allowed values are global or module . If the value is global the variable will be added as-is to the global namespace. If the value is module then the variable name will be prefixed with the module alias (e.g. the flavor variable in the cluster module would be named cluster_flavor with module scope and flavor with global scope). alias The alias name that should be given to the variable in the global variable namespace. This alias works in conjunction with the scope value. For example, if the name variable is set to global scope and alias of my_name then a variable named my_name will be added to the global variable namespace and the generated module terraform will map the my_name global variable to the name module variable ( name = var.my_name ) important Flag that indicates the variable should be presented to the user in the generated *.auto.tfvars file even though it has a default value. By default, only required fields (i.e. fields that don't have a default value) are presented to the user. Selectively, other variables can be exposed using this flag for significant configuration values. The objective is to balance flexibility of configuration options with the simplicity of a small number of required inputs Note: The module metadata defines how the outputs from the dependent modules should be wired into a module's input variables. It is not necessary to define any of the \"wired\" variables in the BOM. Example Bill of Material \u00b6 apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Bill of Material"},{"location":"reference/bom/#bill-of-material-reference","text":"Todo Complete this reference section check the content as moved from previous documentation structure.","title":"Bill of Material Reference"},{"location":"reference/bom/#bill-of-material-reference_1","text":"The Bill Of Materials (BOM) yaml has been modeled after a Kubernetes Custom Resource Definition. It is used to define the modules from the module catalog that should be included in the generated terraform template. As appropriate the Bill of Materials can also be used to define the relationships between the modules and the default variables that should be supplied to the modules for the architecture. The terraform template is generated from the BOM using the iascable build command. The build process relies on metadata for each of the modules stored in the module catalog to understand each module's dependencies and the relationships between the different modules. By default, the module entries for the Bill of Material are pulled from the Cloud Native Toolkit module catalog - https://modules.cloudnativetoolkit.dev/","title":"Bill of Material reference"},{"location":"reference/bom/#bom-metadata","text":"The first part of the BOM defines the name and other descriptive information about the terraform that will be generated. apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 100-shared-services labels : platform : ibm code : '100' annotations : displayName : Shared Services description : Provisions a set of shared services in the IBM Cloud account Note: The labels and annotations sections can contain any number of values. The common values are shown in the example. Field Description apiVersion the schema version of the BOM (always cloudnativetoolkit.dev/v1alpha1 at the moment) kind the kind of resource (always BillOfMaterial for a BOM) name the name of the architecture that will be built platform label the cloud platform targeted by the architecture code label the code used to index the BOM displayName annotation the user-friendly display name for the BOM description annotation the description of the provisioned architecture path annotation the sub-path that should be appended to the output (e.g. {output}/{path}/{name} catalogUrls annotation comma-separated list of urls for the catalogs containing the BOM modules deployment-type/gitops annotation flag indicating the BOM describes gitops modules vpn/required annotation flag indicating a VPN connection is required before applying the terraform","title":"BOM metadata"},{"location":"reference/bom/#bom-spec","text":"The meat of the BOM is defined in the spec block. The spec can contain the following top level elements: modules - an array of Bill of Material module definitions variables - (optional) an array of Bill of Material variables used to define the global variables in the terraform template providers - (optional) an array of terraform provider configurations","title":"BOM spec"},{"location":"reference/bom/#bom-module-definition","text":"A BOM module is used to define a module that should be added to the generated terraform template. At a minimum, the BOM Module must define name of the module from the module catalog. Optionally, the module can also define an alias that will be used for the module identifier in the generated terraform and will also be used as the identifier when defining dependencies between modules.","title":"BOM module definition"},{"location":"reference/bom/#example-bill-of-material","text":"apiVersion : cloudnativetoolkit.dev/v1alpha1 kind : BillOfMaterial metadata : name : 130-management-vpc-openshift labels : type : infrastructure platform : ibm code : '130' annotations : displayName : Management VPC OpenShift description : Management VPC and Red Hat OpenShift servers spec : modules : - name : ibm-resource-group alias : kms_resource_group variables : - name : provision value : false - name : ibm-resource-group alias : at_resource_group variables : - name : provision value : false - name : ibm-kms alias : kms variables : - name : provision value : false - name : region alias : kms_region - name : name_prefix alias : kms_name_prefix scope : global value : \"\" dependencies : - name : resource_group ref : kms_resource_group - name : ibm-resource-group variables : - name : resource_group_name alias : mgmt_resource_group_name scope : global - name : provision alias : mgmt_resource_group_provision scope : global - name : ibm-access-group - name : ibm-vpc variables : - name : address_prefix_count value : 3 - name : address_prefixes value : - 10.10.0.0/18 - 10.20.0.0/18 - 10.30.0.0/18 - name : ibm-flow-logs dependencies : - name : target ref : ibm-vpc - name : cos_bucket ref : flow_log_bucket - name : ibm-vpc-gateways - name : ibm-vpc-subnets alias : worker-subnets variables : - name : _count alias : mgmt_worker_subnet_count scope : global value : 3 - name : label value : worker - name : ipv4_cidr_blocks value : - 10.10.10.0/24 - 10.20.10.0/24 - 10.30.10.0/24 dependencies : - name : gateways ref : ibm-vpc-gateways - name : ibm-ocp-vpc alias : cluster variables : - name : disable_public_endpoint value : true - name : kms_enabled value : true - name : worker_count alias : mgmt_worker_count - name : ocp_version value : 4.8 dependencies : - name : subnets ref : worker-subnets - name : kms_key ref : kms_key - name : ibm-vpc-subnets alias : vpe-subnets variables : - name : _count value : 3 - name : label value : vpe - name : ipv4_cidr_blocks value : - 10.10.20.0/24 - 10.20.20.0/24 - 10.30.20.0/24 - name : ibm-vpc-subnets alias : ingress-subnets variables : - name : _count value : 3 - name : label value : ingress - name : ipv4_cidr_blocks value : - 10.10.30.0/24 - 10.20.30.0/24 - 10.30.30.0/24 - name : ibm-vpc-vpn-gateway dependencies : - name : subnets ref : vpn-subnets - name : ibm-resource-group alias : cs_resource_group variables : - name : provision value : false - name : ibm-object-storage alias : cos variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-kms-key variables : - name : provision value : true dependencies : - name : kms ref : kms - name : ibm-activity-tracker variables : - name : provision value : false dependencies : - name : resource_group ref : at_resource_group - name : ibm-object-storage-bucket alias : flow_log_bucket variables : - name : label value : flow-logs - name : allowed_ip value : - 0.0.0.0/0 - name : ibm-vpe-gateway alias : vpe-cos dependencies : - name : resource ref : cos - name : subnets ref : vpe-subnets - name : sync ref : cluster - name : ibm-transit-gateway variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource-group ref : cs_resource_group - name : logdna variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : sysdig variables : - name : provision value : false - name : name_prefix alias : cs_name_prefix scope : global dependencies : - name : resource_group ref : cs_resource_group - name : ibm-logdna-bind - name : sysdig-bind variables : - name : mgmt_resource_group_name - name : mgmt_resource_group_provision - name : region - name : ibmcloud_api_key - name : name_prefix alias : mgmt_name_prefix required : true - name : cs_resource_group_name - name : cs_name_prefix - name : worker_count - name : kms_service","title":"Example Bill of Material"},{"location":"reference/gitops-structure/","text":"GitOps Repository Structure \u00b6 GitOps describes an approach to managing the deployment and maintenance of an environment through configuration stored in a Git repository. However, GitOps does not dictate a particular way the information in the Git repository is organized. There are a number of different tools and approaches available. For the Toolkit, we had defined an opinionated approach to the GitOps repository structure to address complex deployments and take advantage of the capabilities of available GitOps tools. The following gives an overview of this opinionated GitOps repository structure. App of Apps \u00b6 There are two major types of resources in the repository: ArgoCD configuration Application \"payloads\" ArgoCD configuration \u00b6 In ArgoCD, collections of kubernetes resources that are deployed together are called \"applications\". Applications in ArgoCD are configured using a custom resource definition (CRD) in the cluster which means ArgoCD applications can deploy other ArgoCD applications (called the \"App of Apps pattern\" ). With the \"App of Apps pattern\", the ArgoCD environment can be bootstrapped with an initial application. That initial bootstrap application can then be updated in the GitOps repository to configure other applications. Application \"payloads\" \u00b6 The ArgoCD configuration points to other paths within the GitOps repository (or various GitOps repositories) that contain the actual \"payload\" yaml to provision the applications (the deployments, config maps, etc that make up the applications)/ Layered components \u00b6 In addition to separating the ArgoCD configuration from the application \"payloads\", the configuration has also been divided into three different \"layers\" of the cluster configuration: Infrastructure Shared services Applications Infrastructure \u00b6 Foundational elements within the cluster, like namespaces, service accounts, role-based access control, etc. These resources are often managed by the infrastructure team and are required by the other resources. Shared Services \u00b6 Shared services are application components that are used across multiple applications or across the cluster. Often these are operator-based services and managed independently from the applications. Applications \u00b6 The application layer contains the applications deployed to the cluster, using the infrastructure and shared service components. Structure \u00b6 Putting it all together, there are seven different locations for the GitOps content: Bootstrap Infrastructure ArgoCD configuration Shared services ArgoCD configuration Application ArgoCD configuration Infrastructure payload Shared services payload Application payload This repository implements a simple configuration where all seven collections of resources are stored in a single repository. For more complicated deployments, the resources can be separated into different repositories. For example, if the infrastructure, services, and application configuration is managed by different teams then each layer can be managed in a different gitops repository. argocd/ \u00b6 The argocd/ folder contains the ArgoCD application config, separated into the different layer folders: 0-bootstrap 1-infrastructure 2-services 3-applications The \"bootstrap\" repository must at least contain the argocd/0-bootstrap folder with the initial configuration. The other layer folders can either be defined in the same repository or separate repositories. Multi-server \u00b6 The GitOps repository structure supports defining the deployment configuration for multiple servers in a single Git repository. Each of the argocd configuration paths includes a cluster name/identifier so that the full path to the configuration is argocd/{layer}/cluster/{cluster id} , e.g. argocd/0-bootstrap/cluster/default . The default cluster id used is \"default\". payload/ \u00b6 The payload/ folder contains the kubernetes resources for the application components for the infrastructure, services, and application layers. These folders can be in the bootstrap repository or any other repository. config.yaml \u00b6 In order to understand where all the pieces that make up the GitOps deployment can be located, the bootstrap repository contains a yaml file that defines the repository and path for each of the seven locations. This file can be used both by humans to understand the layout and by other tools to place content in the appropriate location. For example: bootstrap : argocd-config : project : 0-bootstrap repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/0-bootstrap infrastructure : argocd-config : project : 1-infrastructure repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/1-infrastructure payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/1-infrastructure services : argocd-config : project : 2-services repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/2-services payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/2-services applications : argocd-config : project : 3-applications repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/3-applications payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/3-applications kubeseal_cert.pem \u00b6 By default, the Toolkit uses Sealed Secrets to store encrypted credentials in the GitOps repository. Sealed Secrets works by installing the operator in the cluster and configuring it with a private key. Any credentials are encrypted with the corresponding public cert before storing the value in the GitOps repository. The operator then decrypts the credentials in-cluster and creates the relevant kubernetes Secrets. In order to aid the process, the public certificate that can be used to encrypt additional secrets is stored in repository in the kubeseal_cert.pem . Example \u00b6 An example GitOps repository is provided at https://github.com/cloud-native-toolkit-test/gitops-sample","title":"GitOps Structure"},{"location":"reference/gitops-structure/#gitops-repository-structure","text":"GitOps describes an approach to managing the deployment and maintenance of an environment through configuration stored in a Git repository. However, GitOps does not dictate a particular way the information in the Git repository is organized. There are a number of different tools and approaches available. For the Toolkit, we had defined an opinionated approach to the GitOps repository structure to address complex deployments and take advantage of the capabilities of available GitOps tools. The following gives an overview of this opinionated GitOps repository structure.","title":"GitOps Repository Structure"},{"location":"reference/gitops-structure/#app-of-apps","text":"There are two major types of resources in the repository: ArgoCD configuration Application \"payloads\"","title":"App of Apps"},{"location":"reference/gitops-structure/#argocd-configuration","text":"In ArgoCD, collections of kubernetes resources that are deployed together are called \"applications\". Applications in ArgoCD are configured using a custom resource definition (CRD) in the cluster which means ArgoCD applications can deploy other ArgoCD applications (called the \"App of Apps pattern\" ). With the \"App of Apps pattern\", the ArgoCD environment can be bootstrapped with an initial application. That initial bootstrap application can then be updated in the GitOps repository to configure other applications.","title":"ArgoCD configuration"},{"location":"reference/gitops-structure/#application-payloads","text":"The ArgoCD configuration points to other paths within the GitOps repository (or various GitOps repositories) that contain the actual \"payload\" yaml to provision the applications (the deployments, config maps, etc that make up the applications)/","title":"Application \"payloads\""},{"location":"reference/gitops-structure/#layered-components","text":"In addition to separating the ArgoCD configuration from the application \"payloads\", the configuration has also been divided into three different \"layers\" of the cluster configuration: Infrastructure Shared services Applications","title":"Layered components"},{"location":"reference/gitops-structure/#infrastructure","text":"Foundational elements within the cluster, like namespaces, service accounts, role-based access control, etc. These resources are often managed by the infrastructure team and are required by the other resources.","title":"Infrastructure"},{"location":"reference/gitops-structure/#shared-services","text":"Shared services are application components that are used across multiple applications or across the cluster. Often these are operator-based services and managed independently from the applications.","title":"Shared Services"},{"location":"reference/gitops-structure/#applications","text":"The application layer contains the applications deployed to the cluster, using the infrastructure and shared service components.","title":"Applications"},{"location":"reference/gitops-structure/#structure","text":"Putting it all together, there are seven different locations for the GitOps content: Bootstrap Infrastructure ArgoCD configuration Shared services ArgoCD configuration Application ArgoCD configuration Infrastructure payload Shared services payload Application payload This repository implements a simple configuration where all seven collections of resources are stored in a single repository. For more complicated deployments, the resources can be separated into different repositories. For example, if the infrastructure, services, and application configuration is managed by different teams then each layer can be managed in a different gitops repository.","title":"Structure"},{"location":"reference/gitops-structure/#argocd","text":"The argocd/ folder contains the ArgoCD application config, separated into the different layer folders: 0-bootstrap 1-infrastructure 2-services 3-applications The \"bootstrap\" repository must at least contain the argocd/0-bootstrap folder with the initial configuration. The other layer folders can either be defined in the same repository or separate repositories.","title":"argocd/"},{"location":"reference/gitops-structure/#multi-server","text":"The GitOps repository structure supports defining the deployment configuration for multiple servers in a single Git repository. Each of the argocd configuration paths includes a cluster name/identifier so that the full path to the configuration is argocd/{layer}/cluster/{cluster id} , e.g. argocd/0-bootstrap/cluster/default . The default cluster id used is \"default\".","title":"Multi-server"},{"location":"reference/gitops-structure/#payload","text":"The payload/ folder contains the kubernetes resources for the application components for the infrastructure, services, and application layers. These folders can be in the bootstrap repository or any other repository.","title":"payload/"},{"location":"reference/gitops-structure/#configyaml","text":"In order to understand where all the pieces that make up the GitOps deployment can be located, the bootstrap repository contains a yaml file that defines the repository and path for each of the seven locations. This file can be used both by humans to understand the layout and by other tools to place content in the appropriate location. For example: bootstrap : argocd-config : project : 0-bootstrap repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/0-bootstrap infrastructure : argocd-config : project : 1-infrastructure repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/1-infrastructure payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/1-infrastructure services : argocd-config : project : 2-services repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/2-services payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/2-services applications : argocd-config : project : 3-applications repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : argocd/3-applications payload : repo : github.com/cloud-native-toolkit-test/gitops-sample url : https://github.com/cloud-native-toolkit-test/gitops-sample.git path : payload/3-applications","title":"config.yaml"},{"location":"reference/gitops-structure/#kubeseal_certpem","text":"By default, the Toolkit uses Sealed Secrets to store encrypted credentials in the GitOps repository. Sealed Secrets works by installing the operator in the cluster and configuring it with a private key. Any credentials are encrypted with the corresponding public cert before storing the value in the GitOps repository. The operator then decrypts the credentials in-cluster and creates the relevant kubernetes Secrets. In order to aid the process, the public certificate that can be used to encrypt additional secrets is stored in repository in the kubeseal_cert.pem .","title":"kubeseal_cert.pem"},{"location":"reference/gitops-structure/#example","text":"An example GitOps repository is provided at https://github.com/cloud-native-toolkit-test/gitops-sample","title":"Example"},{"location":"reference/glossary/","text":"Glossary \u00b6 A \u00b6 Term Description Ascent The project name for the builder UI B \u00b6 Term Description Bill of Materials Recipe for creating a desired environment BOM Acronym of Bill of Materials I \u00b6 Term Description iascable The command line tool to generate the automation assets from a Bill of Materials M \u00b6 Term Description Module A composable block to setup/install a part of an environment Module catalog The online catalog of available modules","title":"Glossary"},{"location":"reference/glossary/#glossary","text":"","title":"Glossary"},{"location":"reference/glossary/#a","text":"Term Description Ascent The project name for the builder UI","title":"A"},{"location":"reference/glossary/#b","text":"Term Description Bill of Materials Recipe for creating a desired environment BOM Acronym of Bill of Materials","title":"B"},{"location":"reference/glossary/#i","text":"Term Description iascable The command line tool to generate the automation assets from a Bill of Materials","title":"I"},{"location":"reference/glossary/#m","text":"Term Description Module A composable block to setup/install a part of an environment Module catalog The online catalog of available modules","title":"M"},{"location":"reference/module/","text":"Module Reference \u00b6 Todo Complete this reference section module schema module file/directory layout module implementation guidance module testing requirements self-hosted modules (link to catalog reference) A Toolkit module is a Terraform project with some additional metadata. The metadata must be provided in a file called module.yaml , which must conform to the module schema Todo Add description of schema here","title":"Module"},{"location":"reference/module/#module-reference","text":"Todo Complete this reference section module schema module file/directory layout module implementation guidance module testing requirements self-hosted modules (link to catalog reference) A Toolkit module is a Terraform project with some additional metadata. The metadata must be provided in a file called module.yaml , which must conform to the module schema Todo Add description of schema here","title":"Module Reference"},{"location":"resources/overview/","text":"Resources \u00b6 ASCENT Automation bundles Automation runtimes Module catalog Troubleshooting","title":"Overview"},{"location":"resources/overview/#resources","text":"ASCENT Automation bundles Automation runtimes Module catalog Troubleshooting","title":"Resources"},{"location":"resources/ascent/","text":"ASCENT \u00b6","title":"ASCENT"},{"location":"resources/ascent/#ascent","text":"","title":"ASCENT"},{"location":"resources/automation-bundles/","text":"Automation bundles \u00b6","title":"Automation bundles"},{"location":"resources/automation-bundles/#automation-bundles","text":"","title":"Automation bundles"},{"location":"resources/automation-runtimes/","text":"Automation runtime environments \u00b6 Supported runtimes \u00b6 There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled. Docker Desktop \u00b6 Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Multipass \u00b6 Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation. Unsupported runtimes \u00b6 Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed Colima instructions \u00b6 Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation Podman instructions \u00b6 Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode. Getting started with Podman for MacOS \u00b6 Install Brew Install Podman (a replacement for Docker Desktop ) and the docker cli brew install podman docker Create a podman machine, set it to run in rootful mode and start it podman machine init podman machine set --rootful podman machine start Once the podman vm is started, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed. Dealing with known issues for Podman on MacOS \u00b6 When resuming from suspend, if the podman machine is left running, it will not automatically synchronize to the host clock. This will cause the podman machine to lose time. Either stop/restart the podman machine or define an alias like this in your startup scripts: alias fpt = \"podman machine ssh \\\"sudo chronyc -m 'burst 4/4' makestep; date -u\\\"\" then fix podman time with the fpt command. There is currently an QEMU bug which prevents binary files that should be executable by the podman machine vm from operating from inside a mounted volume path. This is most common when using the host automation directory, vs a container volume like /workspaces for running the automation. Generally the cli-tools image will have any binary needed and the utils-cli module will symbolically link, vs. download a new binary into this path. However there can be drift between binaries in cli-tools image used by launch.sh and those requested to the utils-cli module. Getting started with Podman for Linux \u00b6 Visit and follow the installation instructions for your distribution Once the podman application is installed provide sudo podman as the first argument to the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed: ./launch.sh 'sudo podman' More information available at: https://podman.io/getting-started/installation","title":"Automation Runtimes"},{"location":"resources/automation-runtimes/#automation-runtime-environments","text":"","title":"Automation runtime environments"},{"location":"resources/automation-runtimes/#supported-runtimes","text":"There are two supported runtimes where the automation is expected to be executed inside of: Docker Desktop (Container engine) Multipass (VM) The Terraform automation can be run from the local operating system, but it is recommended to use either of the runtimes listed above, which provide a consistent and controlled environment, with all dependencies preinstalled.","title":"Supported runtimes"},{"location":"resources/automation-runtimes/#docker-desktop","text":"Docker Desktop is an easy-to-use application that enables you to build and share containerized applications. It provides a simple interface that enables you to manage your containers, applications, and images directly from your machine without having to use the CLI to perform core actions. Docker Desktop is supported across Mac, Windows, and Linux, and can be downloaded and installed directly from: https://www.docker.com/products/docker-desktop/ Once installed, use the automation template's launch.sh script to launch an interactive shell where the Terraform automation can be executed.","title":"Docker Desktop"},{"location":"resources/automation-runtimes/#multipass","text":"Multipass is a simplified Ubuntu Linux Virtual Machine that you can spin up with a single command. With this option you spin up a virtual machine with a predefined configuration that is ready to run the Terraform automation. You can download and install Multipass from https://multipass.run/install Once you have installed Multipass, open up a command line terminal and cd into the parent directory where you cloned the automation repo. Download the cloud-init script for use by the virtual machine using: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml The cli-tools cloud init script prepares a VM with the same tools available in the quay.io/cloudnativetoolkit/cli-tools-ibmcloud container image. Particularly: terraform terragrunt git jq yq oc kubectl helm ibmcloud cli Launch the Multipass virtual machine using the following command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine and apply the configuration. Once the virtual machine is started, you need to mount the local file system for use within the virtual machine. Then mount the file system using the following command: multipass mount $PWD cli-tools:/automation This will mount the parent directory to the /automation directory inside of the virtual machine. \u26a0\ufe0f MacOS users may encounter the following error if Multipass has not been granted file system access. mount failed: source \"{current directory}\" is not readable If you encounter this error, then you need to enable full disk access in the operating system before you can successfully mount the volume. Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . After granting access to multipassd , then re-run the multipass mount $PWD cli-tools:/automation command. Once the virtual machine has started, run the following command to enter an interactive shell: multipass shell cli-tools Once in the shell, cd into the /automation/{template} folder, where {template} is the Terraform template you configured. Then you need to load credentials into environment variables using the following command: source credentials.properties Once complete, you will be in an interactive shell that is pre-configured with all dependencies necessary to execute the Terraform automation.","title":"Multipass"},{"location":"resources/automation-runtimes/#unsupported-runtimes","text":"Additional container engines, such as podman or colima may be used at your own risk. They may work, however, there are known issues using these environments, and the development team does not provide support for these environments. Known issues include: Network/DNS failures under load Read/write permissions to local storage volumes Issues running binary executables from volumes mounted from the host Time drift issues when hosts are suspended/resumed","title":"Unsupported runtimes"},{"location":"resources/automation-runtimes/#colima-instructions","text":"Install Brew Install Colima (a replacement for Docker Desktop ) and the docker cli brew install colima docker More information available at: https://github.com/abiosoft/colima#installation","title":"Colima instructions"},{"location":"resources/automation-runtimes/#podman-instructions","text":"Unlike Docker which traditionally has separated a cli from a daemon-based container engine, Podman is a daemon-less container engine originally developed for Linux systems. There is a MacOS port which has sufficient features to support running the automation based on container images. Podman can run containers in root or rootless mode. Current permissions setup in the launch.sh script will require root mode.","title":"Podman instructions"},{"location":"resources/module-catalog/","text":"Module catalog \u00b6","title":"Module catalog"},{"location":"resources/module-catalog/#module-catalog","text":"","title":"Module catalog"},{"location":"resources/troubleshooting/","text":"Troubleshooting \u00b6 Uninstalling \u00b6 To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in the README file in the repository Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve Variables may not be used here. \u00b6 You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct Intermittent network failures when using Colima \u00b6 If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures. Resources stuck in Terminating state \u00b6 When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications Workspace permission issues \u00b6 Root user on Linux \u00b6 If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions. Legacy launch.sh script \u00b6 IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README in the repository to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration. That didn't work, what next? \u00b6 If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"Troubleshooting"},{"location":"resources/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"resources/troubleshooting/#uninstalling","text":"To uninstall this solution: Use the ./launch.sh script to enter the docker container that was used to install this software package as described in the README file in the repository Navigate to the /workspaces/current folder: cd /workspaces/current There are 2 ways you can uninstall this solution: Use the /destroy-all.sh script to uninstall all layers of the solution Navigate into the specific subdirectories of the solution and remove specific layers. These steps should be applied for all layers, in reverse order, starting with the highest-numbered layer first. Repeat for all layers/sub-folders in your solution. cd 200-openshift-gitops terraform init terraform destroy --auto-approve","title":"Uninstalling"},{"location":"resources/troubleshooting/#variables-may-not-be-used-here","text":"You may encounter an error message containing Variables may not be used here. during terraform execution, similar to the following: \u2502 Error: Variables not allowed \u2502 \u2502 on terraform.tfvars line 1: \u2502 1: cluster_login_token=asdf \u2502 \u2502 Variables may not be used here. This error happens when values in a tfvars file are not wrapped in quotes. In this case terraform interprets the value as a variable reference, which does not exist. To remedy this situation, wrap the value in your terraform.tfvars in quotes. For example: - cluster_login_token=ABCXYZ is incorrect - cluster_login_token=\"ABCXYZ\" is correct","title":"Variables may not be used here."},{"location":"resources/troubleshooting/#intermittent-network-failures-when-using-colima","text":"If you are using the colima container engine (replacement for Docker Desktop), you may see random network failures when the container is put under heavy network load. This happens when the internal DNS resolver can't keep up with the container's network requests. The workaround is to switch colima to use external DNS instead of it's own internal DNS. Steps to fix this solution: Stop Colima using colima stop Create a file ~/.lima/_config/override.yaml containing the following: useHostResolver: false dns: - 8.8.8.8 Restart Colima using colima start Resume your activities where you encountered networking failures. It may be required to execute a terraform destroy command to cleanup invalid/bad state due to network failures.","title":"Intermittent network failures when using Colima"},{"location":"resources/troubleshooting/#resources-stuck-in-terminating-state","text":"When deleting resources, the namespaces used by the solution occasionally will get stuck in a terminating or inconsistent state. Use the following steps to recover from these conditions: Follow these steps: - run oc get namespace <namespace> -o yaml on the CLI to get the details for the namespace. Within the yaml output, you can see if resources are stuck in a finalizing state. - Get the details of the remaining resource oc get <type> <instance> -n <namespace> -o yaml to see details on the resources that are stuck and have not been cleaned up. The <type> and <instance> can be found in the output of the previous oc get namespace <namespace> -o yaml command. - Patch the instances to remove the stuck finalizer: oc patch <type> <instance> -n <namespace> -p '{\"metadata\": {\"finalizers\": []}}' --type merge - Delete the resource that was stuck: oc delete <type> <instance> -n <namespace> - Go into ArgoCD instance and delete the remaining argo applications","title":"Resources stuck in Terminating state"},{"location":"resources/troubleshooting/#workspace-permission-issues","text":"","title":"Workspace permission issues"},{"location":"resources/troubleshooting/#root-user-on-linux","text":"If you are running on a linux machine as root user, the terraform directory is locked down so that only root had write permissions. When the launch.sh script puts you into the docker container, you are no longer root, and you encounter permission denied errors when executing setupWorkspace.sh . If the user on the host operating system is root , then you have to run chmod g+w -R . before running launch.sh to allow the terraform directory to be group writeable. Once you do this, the permission errors go away, and you can follow the installation instructions.","title":"Root user on Linux"},{"location":"resources/troubleshooting/#legacy-launchsh-script","text":"IF you are not encountering the root user issue described above, and You may encounter permission errors if you have previously executed this terraform automation using an older launch.sh script (prior to June 2022). If you had previously executed the older launch.sh script, it mounted the workspace volume with root as the owner. The current launch.sh script mounts the workspace volume as the user devops . When trying to execute commands, you will encounter permission errors, and terraform or setupWorkspace.sh commands will only work if you use the sudo command. If this is the case, the workaround is to remove the workspace volume on your system, so that it can be recreated with the proper ownership. To do this: Exit the container using the exit command Verify that you have the workspace volume by executing docker volume list Delete the workspace volume using docker volume rm workspace If this command fails, you may first have to remove containers that reference the volume. User docker ps to list containers and docker rm <container> to remove a container. After you delete the container, re-run docker volume rm workspace to delete the workspace volume. Use the launch.sh script reenter the container. Use the setupWorkspace.sh script as described in the README in the repository to reconfigure your workspace and continue with the installation process. You should never use the sudo command to execute this automation. If you have to use sudo , then something is wrong with your configuration.","title":"Legacy launch.sh script"},{"location":"resources/troubleshooting/#that-didnt-work-what-next","text":"If you continue to experience issues with this automation, please file an issue or reach out on our public Discord server .","title":"That didn't work, what next?"},{"location":"tasks/","text":"Tasks \u00b6 Todo Complete this section The tasks section should be split into 2 parts. The guided tasks, using the builder (ascent) UI and the advanced tasks, which drop down to the iascable command line tool. Guided tasks: outline the tasks needed to be able to use a provided Bill of Materials or build and deploy a custom Bill of Materials using the builder UI (ascent) Advanced tasks: outline the tasks needed to be able to build and deploy a Bill of Materials using the iascable CLI tool outline the tasks needed to extend the toolkit by creating, testing and publishing additional modules","title":"Overview"},{"location":"tasks/#tasks","text":"Todo Complete this section The tasks section should be split into 2 parts. The guided tasks, using the builder (ascent) UI and the advanced tasks, which drop down to the iascable command line tool. Guided tasks: outline the tasks needed to be able to use a provided Bill of Materials or build and deploy a custom Bill of Materials using the builder UI (ascent) Advanced tasks: outline the tasks needed to be able to build and deploy a Bill of Materials using the iascable CLI tool outline the tasks needed to extend the toolkit by creating, testing and publishing additional modules","title":"Tasks"},{"location":"tutorials/","text":"Tutorials \u00b6 Todo Complete this section Provide a learning journey to go from setup and 'Hello World' getting started experience to production ready use of the tooling: using builder UI (ascent) using iascable CLI for more advanced use cases creating new modules to extend the toolkit contributing back to community hosting private catalog of modules","title":"Overview"},{"location":"tutorials/#tutorials","text":"Todo Complete this section Provide a learning journey to go from setup and 'Hello World' getting started experience to production ready use of the tooling: using builder UI (ascent) using iascable CLI for more advanced use cases creating new modules to extend the toolkit contributing back to community hosting private catalog of modules","title":"Tutorials"},{"location":"tutorials/1-setup/","text":"Setup you environment \u00b6 This tutorial will prepare your workstation or laptop to be able to deploy a Bill of Material or create a new module to extend the catalog of available modules. Prerequisites \u00b6 The instructions below are written for a modern, up to date version of Linux, MacOS or Windows operating systems. The curl utility is used in some of the instructions in this tutorial. Most OS installations come with curl preinstalled, if not you should install curl before proceeding. Installing the environment \u00b6 Select which environment you want to work with. There is a discussion about these options in the Getting Started section. Linux MacOS Windows Follow the instructions to install the docker engine for your Linux distribution For convenience you may want to enable your Linux user to be able to run docker without using sudo. If so, follow these instructions to add your user to the docker group You may also want to enable docker to start at boot by following these instructions Docker Multipass Docker Desktop can be installed directly from the Docker website Install from the Multipass website Warning After installation you need to give Multipass permission to access the local hard disk: Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . Once installed you need to prepare a virtual machine with the necessary tools installed. To do this you need to: Open a Terminal window Navigate to a local directory you want to work in, using the cd command Download the virtual machine initialization file using command: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Launch the Multipass virtual machine with command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine mount your local filesystem for use within the virtual machine with the following command: multipass mount $PWD cli-tools:/automation Warning If you use a VPN solution such as Cisco Anywhere or TunnelBlick then you may find your multipass VM cannot access the internet. This is due to the packet filter configuration (/etc/pf.conf) on MacOS. You can use Multipass without running the VPN or try one of the following options: Follow this article to configure your packet filter to allow multipass internet traffic when using the VPN if you are using an employer provided workstation or laptop you may need to verify these changes are acceptable with your company security officer Remove Multipass from your system, if already installed, then perform the multipass install and setup with the VPN client running Follow these instructions to install Windows Subsystem for Linux (WSL) and select the Ubuntu 22.04.1 LTS image from the store Open a Ubuntu terminal and follow these instructions to install Docker Engine within the WSL Ubuntu environment For convenience you may want to enable your ubuntu user to be able to run docker without using sudo. If so, follow these instructions to add your user to the docker group Enable docker to start at system boot by running the following command sudo systemctl enable docker.service Start docker with command sudo service docker start Note All command line instructions on this site assume you are working in the WSL environment setup by the above instructions Install local tools \u00b6 Install the following command line tools on your workstation or laptop: oc - The OpenShift command line utility iascable - the Toolkit command line utility to work with Bill of Materials Install oc \u00b6 Follow the instructions in the OpenShift documentation to install the oc cli. Install iascable \u00b6 Iascable is a command line tool to work with Bill of Material files. It can be installed or updated by running the following command: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Setup"},{"location":"tutorials/1-setup/#setup-you-environment","text":"This tutorial will prepare your workstation or laptop to be able to deploy a Bill of Material or create a new module to extend the catalog of available modules.","title":"Setup you environment"},{"location":"tutorials/1-setup/#prerequisites","text":"The instructions below are written for a modern, up to date version of Linux, MacOS or Windows operating systems. The curl utility is used in some of the instructions in this tutorial. Most OS installations come with curl preinstalled, if not you should install curl before proceeding.","title":"Prerequisites"},{"location":"tutorials/1-setup/#installing-the-environment","text":"Select which environment you want to work with. There is a discussion about these options in the Getting Started section. Linux MacOS Windows Follow the instructions to install the docker engine for your Linux distribution For convenience you may want to enable your Linux user to be able to run docker without using sudo. If so, follow these instructions to add your user to the docker group You may also want to enable docker to start at boot by following these instructions Docker Multipass Docker Desktop can be installed directly from the Docker website Install from the Multipass website Warning After installation you need to give Multipass permission to access the local hard disk: Go to System Preferences , then go to Security and Privacy , and select the Privacy tab. Scroll the list on the left and select \"Full Disk Access\" and allow access for multipassd . Once installed you need to prepare a virtual machine with the necessary tools installed. To do this you need to: Open a Terminal window Navigate to a local directory you want to work in, using the cd command Download the virtual machine initialization file using command: curl https://raw.githubusercontent.com/cloud-native-toolkit/sre-utilities/main/cloud-init/cli-tools.yaml --output cli-tools.yaml Launch the Multipass virtual machine with command: multipass launch --name cli-tools --cloud-init ./cli-tools.yaml This will take several minutes to start the virtual machine mount your local filesystem for use within the virtual machine with the following command: multipass mount $PWD cli-tools:/automation Warning If you use a VPN solution such as Cisco Anywhere or TunnelBlick then you may find your multipass VM cannot access the internet. This is due to the packet filter configuration (/etc/pf.conf) on MacOS. You can use Multipass without running the VPN or try one of the following options: Follow this article to configure your packet filter to allow multipass internet traffic when using the VPN if you are using an employer provided workstation or laptop you may need to verify these changes are acceptable with your company security officer Remove Multipass from your system, if already installed, then perform the multipass install and setup with the VPN client running Follow these instructions to install Windows Subsystem for Linux (WSL) and select the Ubuntu 22.04.1 LTS image from the store Open a Ubuntu terminal and follow these instructions to install Docker Engine within the WSL Ubuntu environment For convenience you may want to enable your ubuntu user to be able to run docker without using sudo. If so, follow these instructions to add your user to the docker group Enable docker to start at system boot by running the following command sudo systemctl enable docker.service Start docker with command sudo service docker start Note All command line instructions on this site assume you are working in the WSL environment setup by the above instructions","title":"Installing the environment"},{"location":"tutorials/1-setup/#install-local-tools","text":"Install the following command line tools on your workstation or laptop: oc - The OpenShift command line utility iascable - the Toolkit command line utility to work with Bill of Materials","title":"Install local tools"},{"location":"tutorials/1-setup/#install-oc","text":"Follow the instructions in the OpenShift documentation to install the oc cli.","title":"Install oc"},{"location":"tutorials/1-setup/#install-iascable","text":"Iascable is a command line tool to work with Bill of Material files. It can be installed or updated by running the following command: curl -sL https://iascable.cloudnativetoolkit.dev/install.sh | sh","title":"Install iascable"}]}